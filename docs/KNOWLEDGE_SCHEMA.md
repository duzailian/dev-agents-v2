# AIé©±åŠ¨å›ºä»¶æ™ºèƒ½æµ‹è¯•ç³»ç»Ÿ â€” çŸ¥è¯†åº“æ•°æ®ç»“æ„è®¾è®¡ï¼ˆKNOWLEDGE_SCHEMAï¼‰

> æ–‡æ¡£ç‰ˆæœ¬ï¼šv1.0
>
> ç›®æ ‡ï¼šè®¾è®¡å®Œæ•´çš„çŸ¥è¯†åº“æ•°æ®ç»“æ„ï¼ŒåŒ…å«å‘é‡å­˜å‚¨ã€çŸ¥è¯†å•å…ƒæ¨¡å‹ã€äº§å“çº¿æ ‡ç­¾ä½“ç³»ã€æ•°æ®åº“Schemaå’ŒæŸ¥è¯¢ç¤ºä¾‹ã€‚

---

## 1. è®¾è®¡æ¦‚è¿°

### 1.1 çŸ¥è¯†åº“æ¶æ„

çŸ¥è¯†åº“é‡‡ç”¨æ··åˆå­˜å‚¨æ¶æ„ï¼š
- **å‘é‡æ•°æ®åº“ï¼ˆQdrantï¼‰**ï¼šå­˜å‚¨çŸ¥è¯†å•å…ƒçš„è¯­ä¹‰å‘é‡ï¼Œæ”¯æŒç›¸ä¼¼åº¦æ£€ç´¢
- **å…³ç³»æ•°æ®åº“ï¼ˆPostgreSQLï¼‰**ï¼šå­˜å‚¨ç»“æ„åŒ–å…ƒæ•°æ®ï¼Œæ”¯æŒå¤æ‚å…³è”æŸ¥è¯¢
- **æ–‡ä»¶ç³»ç»Ÿ**ï¼šå­˜å‚¨åŸå§‹æ–‡æ¡£ã€ä»£ç ç‰‡æ®µã€æ—¥å¿—æ–‡ä»¶ç­‰å¤§å¯¹è±¡

### 1.2 æ•°æ®æµå‘

```
ä»£ç ä¿®æ”¹ â†’ TestAgent â†’ æ‰§è¡Œç»“æœ â†’ AnalysisAgent â†’ çŸ¥è¯†æå– â†’ 
KnowledgeUnit â†’ å‘é‡åŒ– â†’ Qdrant + PostgreSQL
```

---

## 2. KnowledgeUnit æ•°æ®æ¨¡å‹

### 2.1 æ ¸å¿ƒæ•°æ®ç»“æ„

```json
{
  "id": "ku_20241227_001",
  "content": {
    "title": "PCIeåˆå§‹åŒ–æ—¶åºä¼˜åŒ–",
    "summary": "é’ˆå¯¹Intel Tiger Lakeå¹³å°ï¼ŒPCIeè®¾å¤‡åœ¨å†·å¯åŠ¨åæœªèƒ½æ­£ç¡®æšä¸¾çš„è§£å†³æ–¹æ¡ˆ",
    "description": "è¯¦ç»†æè¿°é—®é¢˜ç°è±¡ã€æ ¹å› åˆ†æã€è§£å†³æ–¹æ¡ˆ...",
    "code_snippets": [
      {
        "file_path": "drivers/pci/pcie.c",
        "function": "pcie_device_init",
        "language": "c",
        "content": "åŸå§‹ä»£ç ç‰‡æ®µæˆ–ä¿®æ”¹åä»£ç "
      }
    ],
    "modification_details": {
      "change_type": "timing_fix",
      "files_modified": ["drivers/pci/pcie.c", "include/pci.h"],
      "lines_added": 15,
      "lines_removed": 8
    }
  },
  "metadata": {
    "product_line": {
      "soc_type": "Tiger_Lake",
      "firmware_stack": "UEFI",
      "chipset": "HM570",
      "platform": "Server"
    },
    "test_context": {
      "test_environment": "QEMU",
      "test_board": "TGL-QEMU",
      "test_duration": "45min",
      "pass_criteria": "pci_devices_detected == 4"
    },
    "execution_result": {
      "status": "success",
      "execution_time": "2024-12-27T10:30:00Z",
      "iterations_count": 3,
      "success_rate": 0.85
    },
    "tags": ["PCIe", "initialization", "enumeration", "timing"],
    "priority": "high",
    "author": "agent_code_v1.2",
    "confidence_score": 0.92
  },
  "relationships": {
    "related_units": ["ku_20241226_015", "ku_20241225_032"],
    "parent_issue": "issue_20241227_001",
    "test_executions": ["te_20241227_001", "te_20241227_002"]
  },
  "vector_embedding": {
    "model": "text-embedding-ada-002",
    "dimension": 1536,
    "vector": [0.1, -0.2, 0.3, ...]
  },
  "audit": {
    "created_at": "2024-12-27T10:35:00Z",
    "updated_at": "2024-12-27T11:20:00Z",
    "version": "1.0",
    "source": "automated_extraction"
  }
}
```

### 2.2 JSON Schema å®šä¹‰

```json
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "KnowledgeUnit",
  "type": "object",
  "required": ["id", "content", "metadata", "vector_embedding", "audit"],
  "properties": {
    "id": {
      "type": "string",
      "pattern": "^ku_[0-9]{8}_[0-9]{3}$",
      "description": "çŸ¥è¯†å•å…ƒå”¯ä¸€æ ‡è¯†ç¬¦ï¼šku_YYYYMMDD_NNN"
    },
    "content": {
      "type": "object",
      "required": ["title", "summary", "description"],
      "properties": {
        "title": {"type": "string", "maxLength": 200},
        "summary": {"type": "string", "maxLength": 500},
        "description": {"type": "string"},
        "code_snippets": {
          "type": "array",
          "items": {
            "type": "object",
            "required": ["file_path", "language", "content"],
            "properties": {
              "file_path": {"type": "string"},
              "function": {"type": "string"},
              "language": {"type": "string", "enum": ["c", "cpp", "assembly", "python", "shell"]},
              "content": {"type": "string"}
            }
          }
        },
        "modification_details": {
          "type": "object",
          "properties": {
            "change_type": {
              "type": "string",
              "enum": ["bug_fix", "feature_add", "performance", "timing_fix", "refactor"]
            },
            "files_modified": {"type": "array", "items": {"type": "string"}},
            "lines_added": {"type": "integer", "minimum": 0},
            "lines_removed": {"type": "integer", "minimum": 0}
          }
        }
      }
    },
    "metadata": {
      "type": "object",
      "required": ["product_line", "test_context", "execution_result", "tags"],
      "properties": {
        "product_line": {
          "type": "object",
          "required": ["soc_type", "firmware_stack"],
          "properties": {
            "soc_type": {"type": "string"},
            "firmware_stack": {"type": "string"},
            "chipset": {"type": "string"},
            "platform": {"type": "string", "enum": ["Server", "Desktop", "Embedded", "Mobile"]}
          }
        },
        "test_context": {
          "type": "object",
          "properties": {
            "test_environment": {"type": "string"},
            "test_board": {"type": "string"},
            "test_duration": {"type": "string"},
            "pass_criteria": {"type": "string"}
          }
        },
        "execution_result": {
          "type": "object",
          "required": ["status", "execution_time"],
          "properties": {
            "status": {"type": "string", "enum": ["success", "failure", "timeout", "error"]},
            "execution_time": {"type": "string", "format": "date-time"},
            "iterations_count": {"type": "integer", "minimum": 1},
            "success_rate": {"type": "number", "minimum": 0, "maximum": 1}
          }
        },
        "tags": {
          "type": "array",
          "items": {"type": "string"},
          "minItems": 1
        },
        "priority": {
          "type": "string",
          "enum": ["low", "medium", "high", "critical"],
          "default": "medium"
        },
        "author": {"type": "string"},
        "confidence_score": {"type": "number", "minimum": 0, "maximum": 1},
        "verification_status": {
          "type": "string",
          "enum": ["pending_verify", "verified", "rejected", "deprecated"],
          "default": "pending_verify",
          "description": "çŸ¥è¯†éªŒè¯çŠ¶æ€ï¼špending_verify=è‡ªåŠ¨æå–å¾…éªŒè¯ï¼Œverified=äººå·¥æˆ–å¤šæ¬¡éªŒè¯é€šè¿‡ï¼Œrejected=éªŒè¯å¤±è´¥ï¼Œdeprecated=å·²è¿‡æ—¶"
        },
        "maturity_level": {
          "type": "integer",
          "minimum": 0,
          "maximum": 5,
          "default": 0,
          "description": "çŸ¥è¯†æˆç†Ÿåº¦ï¼š0=æ–°å»ºï¼Œ1=å•æ¬¡éªŒè¯ï¼Œ2=å¤šæ¬¡éªŒè¯ï¼Œ3=è·¨äº§å“çº¿éªŒè¯ï¼Œ4=ç”Ÿäº§éªŒè¯ï¼Œ5=æ ‡å‡†åŒ–"
        },
        "verification_history": {
          "type": "array",
          "description": "éªŒè¯å†å²è®°å½•ï¼Œé˜²æ­¢é”™è¯¯çŸ¥è¯†æ±¡æŸ“çŸ¥è¯†åº“",
          "items": {
            "type": "object",
            "properties": {
              "verified_at": {"type": "string", "format": "date-time"},
              "verified_by": {"type": "string"},
              "verification_type": {"type": "string", "enum": ["auto", "manual", "production"]},
              "result": {"type": "string", "enum": ["pass", "fail"]},
              "notes": {"type": "string"}
            }
          }
        },
        "usage_stats": {
          "type": "object",
          "description": "ä½¿ç”¨ç»Ÿè®¡ï¼Œç”¨äºçŸ¥è¯†è€åŒ–å’Œæƒé‡è°ƒæ•´",
          "properties": {
            "total_retrievals": {"type": "integer", "minimum": 0, "default": 0},
            "successful_applications": {"type": "integer", "minimum": 0, "default": 0},
            "failed_applications": {"type": "integer", "minimum": 0, "default": 0},
            "last_used_at": {"type": "string", "format": "date-time"},
            "effectiveness_score": {"type": "number", "minimum": 0, "maximum": 1}
          }
        }
      }
    },
    "relationships": {
      "type": "object",
      "properties": {
        "related_units": {
          "type": "array",
          "items": {"type": "string", "pattern": "^ku_[0-9]{8}_[0-9]{3}$"}
        },
        "parent_issue": {"type": "string"},
        "test_executions": {
          "type": "array",
          "items": {"type": "string", "pattern": "^te_[0-9]{8}_[0-9]{3}$"}
        }
      }
    },
    "vector_embedding": {
      "type": "object",
      "required": ["model", "dimension", "vector"],
      "properties": {
        "model": {"type": "string"},
        "dimension": {"type": "integer", "minimum": 1},
        "vector": {
          "type": "array",
          "items": {"type": "number"},
          "minItems": 1
        }
      }
    },
    "audit": {
      "type": "object",
      "required": ["created_at", "version", "source"],
      "properties": {
        "created_at": {"type": "string", "format": "date-time"},
        "updated_at": {"type": "string", "format": "date-time"},
        "version": {"type": "string"},
        "source": {"type": "string", "enum": ["manual", "automated_extraction", "import"]}
      }
    }
  }
}
```

---

## 3. äº§å“çº¿æ ‡ç­¾ä½“ç³»

### 3.1 SoC Type æ ‡ç­¾å®šä¹‰

```yaml
soc_types:
  Intel:
    - "Tiger_Lake"
    - "Alder_Lake" 
    - "Raptor_Lake"
    - "Meteor_Lake"
    - "Arrow_Lake"
    - "Ice_Lake"
    - "Coffee_Lake"
    - "Kaby_Lake"
    - "Broadwell"
    - "Haswell"
    
  AMD:
    - "EPYC_Milan"
    - "EPYC_Genoa"
    - "EPYC_Bergamo"
    - "Ryzen_7000"
    - "Ryzen_5000"
    - "Ryzen_3000"
    
  ARM:
    - "Cortex_A78"
    - "Cortex_A77"
    - "Cortex_A76"
    - "Neoverse_N1"
    - "Neoverse_V1"
    
  Qualcomm:
    - "Snapdragon_8Gen2"
    - "Snapdragon_7Gen1"
    - "Snapdragon_6Gen1"
    
  Other:
    - "Generic_x86_64"
    - "Generic_AArch64"
    - "RISC_V"
```

### 3.2 Firmware Stack æ ‡ç­¾å®šä¹‰

```yaml
firmware_stacks:
  UEFI:
    - "UEFI_2.8"
    - "UEFI_2.9"
    - "UEFI_3.0"
    - "EDK2"
    - "Aptio"

  ARM_TF:  # ARM Trusted Firmware
    - "ARM_TF_2.8"
    - "ARM_TF_2.9"
    - "ARM_TF_2.10"
    - "ARM_TF_Custom"

  RTOS:  # Real-Time Operating Systems
    - "FreeRTOS"
    - "Zephyr"
    - "ThreadX"
    - "VxWorks"
    - "RT-Thread"
    - "uC/OS"

  UBOOT:  # U-Boot Bootloader
    - "UBOOT_2023"
    - "UBOOT_2024"
    - "UBOOT_SPL"
    - "UBOOT_Custom"
    - "UEFI_3.0"
    - "EDK2"
    - "Aptio"
    
  Legacy_BIOS:
    - "Legacy_BIOS"
    - "Coreboot"
    
  BMC_Firmware:
    - "OpenBMC"
    - "IPMI"
    - "Redfish"
    - "AMI_BMC"
    
  Operating_System:
    - "Linux"
    - "FreeRTOS"
    - "Zephyr"
    - "Bare_Metal"
    
  Bootloader:
    - "GRUB"
    - "U-Boot"
    - "SPL"
    - "Bootloader"
```

### 3.3 æ ‡ç­¾ä½“ç³» JSON Schema

```json
{
  "type": "object",
  "properties": {
    "soc_type": {
      "type": "string",
      "enum": [
        "Tiger_Lake", "Alder_Lake", "Raptor_Lake", "Meteor_Lake",
        "EPYC_Milan", "EPYC_Genoa", "Ryzen_7000", "Cortex_A78",
        "Snapdragon_8Gen2", "Generic_x86_64", "Generic_AArch64"
      ]
    },
    "firmware_stack": {
      "type": "string",
      "enum": [
        "UEFI_2.8", "UEFI_2.9", "UEFI_3.0", "EDK2", "Aptio",
        "ARM_TF", "ARM_TF_2.8", "ARM_TF_2.9", "ARM_TF_2.10",
        "RTOS", "FreeRTOS", "Zephyr", "ThreadX", "VxWorks",
        "UBOOT", "UBOOT_2023", "UBOOT_2024", "UBOOT_SPL",
        "Legacy_BIOS", "Coreboot", "OpenBMC", "IPMI", "Redfish",
        "Linux", "FreeRTOS", "Zephyr", "GRUB", "U-Boot"
      ]
    },
    "chipset": {
      "type": "string",
      "examples": ["HM570", "WM590", "TRX40", "X570", "B450"]
    },
    "platform": {
      "type": "string",
      "enum": ["Server", "Desktop", "Embedded", "Mobile"]
    }
  },
  "required": ["soc_type", "firmware_stack"]
}
```

## 3.4 äº§å“çº¿éš”ç¦»ä¸æ£€ç´¢ç­–ç•¥

### 3.4.1 ProductLineProfileæ•°æ®æ¨¡å‹

ä¸ºè§£å†³å¤šäº§å“çº¿çŸ¥è¯†åº“çš„é«˜æ•ˆæ£€ç´¢é—®é¢˜ï¼Œè®¾è®¡äº†ProductLineProfileæ¨¡å‹ï¼š

```python
from dataclasses import dataclass
from typing import Dict, List, Optional, Union
from enum import Enum
import json

class CompatibilityLevel(Enum):
    """äº§å“çº¿å…¼å®¹æ€§çº§åˆ«"""
    EXACT = "exact"           # å®Œå…¨åŒ¹é…
    FAMILY = "family"         # åŒç³»åˆ—äº§å“å…¼å®¹
    ARCH = "architecture"      # æ¶æ„çº§å…¼å®¹
    GENERIC = "generic"       # é€šç”¨è§£å†³æ–¹æ¡ˆ

@dataclass
class ProductLineProfile:
    """äº§å“çº¿æ¡£æ¡ˆ"""
    profile_id: str
    product_line_tags: Dict[str, Union[str, List[str]]]
    retrieval_priority: int  # 1-10, 10ä¸ºæœ€é«˜
    weight_multipliers: Dict[str, float]
    compatibility_matrix: Dict[str, CompatibilityLevel]
    
    # æ£€ç´¢æ€§èƒ½ä¼˜åŒ–
    cache_ttl: int = 3600  # ç¼“å­˜æ—¶é—´ï¼ˆç§’ï¼‰
    max_candidates: int = 100  # æœ€å¤§å€™é€‰ç»“æœæ•°
    
    # è´¨é‡æ§åˆ¶
    min_confidence_score: float = 0.7
    required_success_rate: float = 0.8
    
    def to_dict(self) -> Dict:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            "profile_id": self.profile_id,
            "product_line_tags": self.product_line_tags,
            "retrieval_priority": self.retrieval_priority,
            "weight_multipliers": self.weight_multipliers,
            "compatibility_matrix": {
                k: v.value for k, v in self.compatibility_matrix.items()
            },
            "cache_ttl": self.cache_ttl,
            "max_candidates": self.max_candidates,
            "min_confidence_score": self.min_confidence_score,
            "required_success_rate": self.required_success_rate
        }

# å…¸å‹äº§å“çº¿æ¡£æ¡ˆç¤ºä¾‹
TIGER_LAKE_PROFILE = ProductLineProfile(
    profile_id="tiger_lake_uefi_server",
    product_line_tags={
        "soc_type": "Tiger_Lake",
        "firmware_stack": "UEFI_2.8",
        "chipset": ["HM570", "WM590"],
        "platform": "Server"
    },
    retrieval_priority=9,
    weight_multipliers={
        "soc_type": 1.0,
        "firmware_stack": 0.9,
        "chipset": 0.8,
        "platform": 0.7
    },
    compatibility_matrix={
        "Tiger_Lake": CompatibilityLevel.EXACT,
        "Alder_Lake": CompatibilityLevel.FAMILY,
        "Generic_x86_64": CompatibilityLevel.ARCH,
        "UEFI_2.9": CompatibilityLevel.FAMILY,
        "EDK2": CompatibilityLevel.ARCH
    },
    cache_ttl=7200,  # 2å°æ—¶ç¼“å­˜
    max_candidates=50,
    min_confidence_score=0.8,
    required_success_rate=0.85
)

EPYC_PROFILE = ProductLineProfile(
    profile_id="epyc_bmc_server",
    product_line_tags={
        "soc_type": "EPYC_Milan",
        "firmware_stack": ["OpenBMC", "UEFI_2.8"],
        "chipset": ["TRX40", "X570"],
        "platform": "Server"
    },
    retrieval_priority=10,
    weight_multipliers={
        "soc_type": 1.0,
        "firmware_stack": 0.95,
        "chipset": 0.75,
        "platform": 0.6
    },
    compatibility_matrix={
        "EPYC_Milan": CompatibilityLevel.EXACT,
        "EPYC_Genoa": CompatibilityLevel.FAMILY,
        "EPYC_Bergamo": CompatibilityLevel.FAMILY,
        "OpenBMC": CompatibilityLevel.EXACT,
        "IPMI": CompatibilityLevel.FAMILY
    },
    cache_ttl=10800,  # 3å°æ—¶ç¼“å­˜
    max_candidates=80,
    min_confidence_score=0.85,
    required_success_rate=0.9
)

GENERIC_PROFILE = ProductLineProfile(
    profile_id="generic_x86_64",
    product_line_tags={
        "soc_type": "Generic_x86_64",
        "firmware_stack": "Generic",
        "platform": "Generic"
    },
    retrieval_priority=1,
    weight_multipliers={
        "soc_type": 0.5,
        "firmware_stack": 0.4,
        "platform": 0.3
    },
    compatibility_matrix={
        "Generic_x86_64": CompatibilityLevel.EXACT,
        "Generic": CompatibilityLevel.EXACT
    },
    cache_ttl=1800,  # 30åˆ†é’Ÿç¼“å­˜
    max_candidates=20,
    min_confidence_score=0.6,
    required_success_rate=0.7
)
```

### 3.4.2 ProductLineMatcherç®—æ³•

```python
import numpy as np
from typing import Tuple, List, Dict
from dataclasses import dataclass

@dataclass
class MatchResult:
    """åŒ¹é…ç»“æœ"""
    knowledge_unit_id: str
    similarity_score: float
    compatibility_level: CompatibilityLevel
    matched_tags: Dict[str, str]
    confidence_adjustment: float

class ProductLineMatcher:
    """äº§å“çº¿åŒ¹é…å™¨"""
    
    def __init__(self, profile: ProductLineProfile):
        self.profile = profile
        self.tag_weights = profile.weight_multipliers
        
    def calculate_tag_similarity(self, 
                                unit_tags: Dict[str, str], 
                                profile_tags: Dict[str, Union[str, List[str]]]) -> float:
        """è®¡ç®—æ ‡ç­¾ç›¸ä¼¼åº¦"""
        total_weight = 0.0
        weighted_score = 0.0
        
        for tag_type, profile_value in profile_tags.items():
            if tag_type not in unit_tags:
                continue
                
            weight = self.tag_weights.get(tag_type, 0.5)
            unit_value = unit_tags[tag_type]
            
            # ç²¾ç¡®åŒ¹é…
            if isinstance(profile_value, str):
                if unit_value == profile_value:
                    weighted_score += weight * 1.0
                elif unit_value in self.profile.compatibility_matrix:
                    compatibility = self.profile.compatibility_matrix[unit_value]
                    weighted_score += weight * self._get_compatibility_score(compatibility)
                else:
                    weighted_score += weight * 0.0
            
            # åˆ—è¡¨åŒ¹é…
            elif isinstance(profile_value, list):
                if unit_value in profile_value:
                    weighted_score += weight * 1.0
                else:
                    # æŸ¥æ‰¾å…¼å®¹æ€§
                    for compatible_value in profile_value:
                        if compatible_value in self.profile.compatibility_matrix:
                            compatibility = self.profile.compatibility_matrix[compatible_value]
                            weighted_score += weight * self._get_compatibility_score(compatibility)
                            break
                    else:
                        weighted_score += weight * 0.0
            
            total_weight += weight
        
        return weighted_score / total_weight if total_weight > 0 else 0.0
    
    def _get_compatibility_score(self, level: CompatibilityLevel) -> float:
        """è·å–å…¼å®¹æ€§è¯„åˆ†"""
        compatibility_scores = {
            CompatibilityLevel.EXACT: 1.0,
            CompatibilityLevel.FAMILY: 0.8,
            CompatibilityLevel.ARCH: 0.6,
            CompatibilityLevel.GENERIC: 0.4
        }
        return compatibility_scores.get(level, 0.0)
    
    def match_knowledge_unit(self, unit: Dict) -> MatchResult:
        """åŒ¹é…å•ä¸ªçŸ¥è¯†å•å…ƒ"""
        unit_tags = unit.get("metadata", {}).get("product_line", {})
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarity_score = self.calculate_tag_similarity(unit_tags, self.profile.product_line_tags)
        
        # ç¡®å®šå…¼å®¹æ€§çº§åˆ«
        compatibility_level = self._determine_compatibility_level(unit_tags)
        
        # è¯†åˆ«åŒ¹é…çš„æ ‡ç­¾
        matched_tags = self._find_matched_tags(unit_tags)
        
        # è®¡ç®—ç½®ä¿¡åº¦è°ƒæ•´
        confidence_adjustment = self._calculate_confidence_adjustment(
            similarity_score, compatibility_level
        )
        
        return MatchResult(
            knowledge_unit_id=unit.get("id"),
            similarity_score=similarity_score,
            compatibility_level=compatibility_level,
            matched_tags=matched_tags,
            confidence_adjustment=confidence_adjustment
        )
    
    def _determine_compatibility_level(self, unit_tags: Dict[str, str]) -> CompatibilityLevel:
        """ç¡®å®šå…¼å®¹æ€§çº§åˆ«"""
        soc_type = unit_tags.get("soc_type")
        if soc_type and soc_type in self.profile.compatibility_matrix:
            return self.profile.compatibility_matrix[soc_type]
        
        # æ£€æŸ¥ firmware_stack å…¼å®¹æ€§
        firmware_stack = unit_tags.get("firmware_stack")
        if firmware_stack and firmware_stack in self.profile.compatibility_matrix:
            return self.profile.compatibility_matrix[firmware_stack]
        
        return CompatibilityLevel.GENERIC
    
    def _find_matched_tags(self, unit_tags: Dict[str, str]) -> Dict[str, str]:
        """æŸ¥æ‰¾åŒ¹é…çš„æ ‡ç­¾"""
        matched = {}
        for tag_type, unit_value in unit_tags.items():
            profile_value = self.profile.product_line_tags.get(tag_type)
            
            if isinstance(profile_value, str) and unit_value == profile_value:
                matched[tag_type] = unit_value
            elif isinstance(profile_value, list) and unit_value in profile_value:
                matched[tag_type] = unit_value
            elif unit_value in self.profile.compatibility_matrix:
                matched[tag_type] = unit_value
        
        return matched
    
    def _calculate_confidence_adjustment(self, 
                                       similarity_score: float, 
                                       compatibility_level: CompatibilityLevel) -> float:
        """è®¡ç®—ç½®ä¿¡åº¦è°ƒæ•´"""
        base_adjustment = {
            CompatibilityLevel.EXACT: 1.0,
            CompatibilityLevel.FAMILY: 0.9,
            CompatibilityLevel.ARCH: 0.8,
            CompatibilityLevel.GENERIC: 0.6
        }.get(compatibility_level, 0.5)
        
        # åŸºäºç›¸ä¼¼åº¦è¿›ä¸€æ­¥è°ƒæ•´
        return base_adjustment * similarity_score

def batch_match_knowledge_units(units: List[Dict], 
                              profile: ProductLineProfile,
                              min_score: float = 0.3) -> List[MatchResult]:
    """æ‰¹é‡åŒ¹é…çŸ¥è¯†å•å…ƒ"""
    matcher = ProductLineMatcher(profile)
    results = []
    
    for unit in units:
        try:
            result = matcher.match_knowledge_unit(unit)
            if result.similarity_score >= min_score:
                results.append(result)
        except Exception as e:
            print(f"åŒ¹é…çŸ¥è¯†å•å…ƒ {unit.get('id', 'unknown')} æ—¶å‡ºé”™: {e}")
            continue
    
    # æŒ‰ç›¸ä¼¼åº¦æ’åº
    results.sort(key=lambda x: x.similarity_score, reverse=True)
    return results
```

### 3.4.3 RetrievalStrategyé…ç½®

```python
from enum import Enum
from typing import Callable, Dict, Any
import time
import hashlib

class RetrievalMode(Enum):
    """æ£€ç´¢æ¨¡å¼"""
    STRICT = "strict"          # ä¸¥æ ¼åŒ¹é…
    BALANCED = "balanced"      # å¹³è¡¡æ¨¡å¼
    BROAD = "broad"           # å®½æ³›åŒ¹é…
    EXPLORATORY = "exploratory"  # æ¢ç´¢æ€§æ£€ç´¢

class CacheStrategy(Enum):
    """ç¼“å­˜ç­–ç•¥"""
    NONE = "none"
    MEMORY = "memory"
    PERSISTENT = "persistent"
    HYBRID = "hybrid"

@dataclass
class RetrievalStrategy:
    """æ£€ç´¢ç­–ç•¥é…ç½®"""
    strategy_id: str
    mode: RetrievalMode
    cache_strategy: CacheStrategy
    max_results: int = 50
    timeout_seconds: int = 30
    
    # æ£€ç´¢è·¯å¾„é…ç½®
    retrieval_paths: List[str] = None
    
    # åŠ¨æ€å‚æ•°
    confidence_threshold: float = 0.7
    similarity_threshold: float = 0.6
    
    # æ€§èƒ½ä¼˜åŒ–
    enable_parallel_search: bool = True
    max_concurrent_requests: int = 5
    
    # è´¨é‡æ§åˆ¶
    min_success_rate: float = 0.8
    max_age_days: int = 365
    
    def __post_init__(self):
        if self.retrieval_paths is None:
            self.retrieval_paths = [
                "exact_match",
                "family_match", 
                "architecture_match",
                "generic_match"
            ]
    
    def get_cache_key(self, query: str, profile_id: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        content = f"{query}:{profile_id}:{self.strategy_id}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def should_use_cache(self) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä½¿ç”¨ç¼“å­˜"""
        return self.cache_strategy in [CacheStrategy.MEMORY, CacheStrategy.HYBRID, CacheStrategy.PERSISTENT]

# é¢„å®šä¹‰æ£€ç´¢ç­–ç•¥
STRICT_RETRIEVAL = RetrievalStrategy(
    strategy_id="strict_tiger_lake",
    mode=RetrievalMode.STRICT,
    cache_strategy=CacheStrategy.PERSISTENT,
    max_results=20,
    confidence_threshold=0.8,
    similarity_threshold=0.7,
    min_success_rate=0.9,
    retrieval_paths=["exact_match", "family_match"]
)

BALANCED_RETRIEVAL = RetrievalStrategy(
    strategy_id="balanced_server",
    mode=RetrievalMode.BALANCED,
    cache_strategy=CacheStrategy.HYBRID,
    max_results=50,
    confidence_threshold=0.7,
    similarity_threshold=0.6,
    min_success_rate=0.8,
    enable_parallel_search=True,
    max_concurrent_requests=3
)

BROAD_RETRIEVAL = RetrievalStrategy(
    strategy_id="broad_exploration",
    mode=RetrievalMode.BROAD,
    cache_strategy=CacheStrategy.MEMORY,
    max_results=100,
    confidence_threshold=0.5,
    similarity_threshold=0.4,
    min_success_rate=0.6,
    retrieval_paths=["exact_match", "family_match", "architecture_match", "generic_match"]
)

class KnowledgeRetrievalEngine:
    """çŸ¥è¯†æ£€ç´¢å¼•æ“"""
    
    def __init__(self, profile: ProductLineProfile, strategy: RetrievalStrategy):
        self.profile = profile
        self.strategy = strategy
        self.cache = {} if strategy.cache_strategy != CacheStrategy.NONE else None
        
    async def retrieve_knowledge(self, 
                                query_text: str, 
                                additional_filters: Dict = None) -> List[Dict]:
        """æ£€ç´¢çŸ¥è¯†"""
        start_time = time.time()
        
        # æ£€æŸ¥ç¼“å­˜
        if self.strategy.should_use_cache():
            cache_key = self.strategy.get_cache_key(query_text, self.profile.profile_id)
            if cache_key in self.cache:
                print(f"ç¼“å­˜å‘½ä¸­: {cache_key}")
                return self.cache[cache_key]
        
        # æ‰§è¡Œæ£€ç´¢
        results = await self._execute_retrieval(query_text, additional_filters)
        
        # åº”ç”¨è´¨é‡è¿‡æ»¤
        filtered_results = self._apply_quality_filters(results)
        
        # ç¼“å­˜ç»“æœ
        if self.strategy.should_use_cache():
            cache_key = self.strategy.get_cache_key(query_text, self.profile.profile_id)
            self.cache[cache_key] = filtered_results
        
        retrieval_time = time.time() - start_time
        print(f"æ£€ç´¢å®Œæˆ: {len(filtered_results)} ä¸ªç»“æœï¼Œè€—æ—¶ {retrieval_time:.2f}s")
        
        return filtered_results
    
    async def _execute_retrieval(self, 
                                query_text: str, 
                                additional_filters: Dict = None) -> List[Dict]:
        """æ‰§è¡Œå…·ä½“æ£€ç´¢é€»è¾‘"""
        # è¿™é‡Œæ˜¯æ£€ç´¢çš„æ ¸å¿ƒå®ç°
        # éœ€è¦é›†æˆå‘é‡æ£€ç´¢å’Œå…³ç³»æ•°æ®åº“æŸ¥è¯¢
        # ç¤ºä¾‹å®ç°ï¼š
        
        results = []
        
        # 1. å‘é‡æ£€ç´¢
        vector_results = await self._vector_search(query_text)
        results.extend(vector_results)
        
        # 2. äº§å“çº¿åŒ¹é…
        matched_results = await self._apply_product_line_matching(results)
        results = matched_results
        
        # 3. åº”ç”¨é¢å¤–è¿‡æ»¤
        if additional_filters:
            results = self._apply_additional_filters(results, additional_filters)
        
        return results
    
    async def _vector_search(self, query_text: str) -> List[Dict]:
        """å‘é‡æœç´¢ï¼ˆæ¨¡æ‹Ÿå®ç°ï¼‰"""
        # å®é™…å®ç°ä¸­ä¼šè°ƒç”¨Qdrant
        print(f"æ‰§è¡Œå‘é‡æœç´¢: {query_text}")
        return []
    
    async def _apply_product_line_matching(self, candidates: List[Dict]) -> List[Dict]:
        """åº”ç”¨äº§å“çº¿åŒ¹é…"""
        if not candidates:
            return []
        
        # ä½¿ç”¨ProductLineMatcherè¿›è¡ŒåŒ¹é…
        matcher = ProductLineMatcher(self.profile)
        match_results = []
        
        for candidate in candidates:
            try:
                match_result = matcher.match_knowledge_unit(candidate)
                # è°ƒæ•´ç½®ä¿¡åº¦
                original_confidence = candidate.get("metadata", {}).get("confidence_score", 0.5)
                adjusted_confidence = original_confidence * match_result.confidence_adjustment
                
                candidate["metadata"]["confidence_score"] = adjusted_confidence
                candidate["metadata"]["similarity_score"] = match_result.similarity_score
                candidate["metadata"]["compatibility_level"] = match_result.compatibility_level.value
                candidate["metadata"]["matched_tags"] = match_result.matched_tags
                
                match_results.append(candidate)
            except Exception as e:
                print(f"åŒ¹é…çŸ¥è¯†å•å…ƒ {candidate.get('id', 'unknown')} æ—¶å‡ºé”™: {e}")
                continue
        
        # æŒ‰ç›¸ä¼¼åº¦å’Œç½®ä¿¡åº¦æ’åº
        match_results.sort(
            key=lambda x: (
                x.get("metadata", {}).get("similarity_score", 0),
                x.get("metadata", {}).get("confidence_score", 0)
            ),
            reverse=True
        )
        
        return match_results[:self.strategy.max_results]
    
    def _apply_quality_filters(self, results: List[Dict]) -> List[Dict]:
        """åº”ç”¨è´¨é‡è¿‡æ»¤"""
        filtered = []
        
        for result in results:
            metadata = result.get("metadata", {})
            
            # ç½®ä¿¡åº¦è¿‡æ»¤
            confidence = metadata.get("confidence_score", 0)
            if confidence < self.strategy.confidence_threshold:
                continue
            
            # ç›¸ä¼¼åº¦è¿‡æ»¤
            similarity = metadata.get("similarity_score", 0)
            if similarity < self.strategy.similarity_threshold:
                continue
            
            # æˆåŠŸç‡è¿‡æ»¤
            execution_result = metadata.get("execution_result", {})
            success_rate = execution_result.get("success_rate", 0)
            if success_rate < self.strategy.min_success_rate:
                continue
            
            # å¹´é¾„è¿‡æ»¤
            execution_time = execution_result.get("execution_time")
            if execution_time:
                age_days = (time.time() - execution_time.timestamp()) / (24 * 3600)
                if age_days > self.strategy.max_age_days:
                    continue
            
            filtered.append(result)
        
        return filtered[:self.strategy.max_results]
    
    def _apply_additional_filters(self, results: List[Dict], filters: Dict) -> List[Dict]:
        """åº”ç”¨é¢å¤–è¿‡æ»¤æ¡ä»¶"""
        filtered = results
        
        for filter_key, filter_value in filters.items():
            filtered = [
                result for result in filtered
                if self._matches_filter(result, filter_key, filter_value)
            ]
        
        return filtered
    
    def _matches_filter(self, result: Dict, filter_key: str, filter_value: Any) -> bool:
        """æ£€æŸ¥æ˜¯å¦åŒ¹é…è¿‡æ»¤æ¡ä»¶"""
        # ç®€åŒ–çš„è¿‡æ»¤é€»è¾‘å®ç°
        # å®é™…ä¸­éœ€è¦æ›´å¤æ‚çš„è¿‡æ»¤è§„åˆ™
        return True
```

### 3.4.4 æ£€ç´¢æ€§èƒ½æŒ‡æ ‡

```python
@dataclass
class RetrievalMetrics:
    """æ£€ç´¢æ€§èƒ½æŒ‡æ ‡"""
    total_results: int
    retrieval_time: float
    cache_hit_rate: float
    avg_confidence: float
    avg_similarity: float
    success_rate: float
    
    # åˆ†å¸ƒæŒ‡æ ‡
    confidence_distribution: Dict[str, int]  # high/medium/low
    compatibility_distribution: Dict[str, int]
    
    # æ€§èƒ½æŒ‡æ ‡
    throughput: float  # ç»“æœ/ç§’
    memory_usage: float  # MB
    cache_size: int

def calculate_retrieval_metrics(results: List[Dict], 
                              retrieval_time: float,
                              cache_info: Dict = None) -> RetrievalMetrics:
    """è®¡ç®—æ£€ç´¢æŒ‡æ ‡"""
    if not results:
        return RetrievalMetrics(
            total_results=0,
            retrieval_time=retrieval_time,
            cache_hit_rate=cache_info.get("hit_rate", 0) if cache_info else 0,
            avg_confidence=0,
            avg_similarity=0,
            success_rate=0,
            confidence_distribution={"high": 0, "medium": 0, "low": 0},
            compatibility_distribution={},
            throughput=0,
            memory_usage=0,
            cache_size=cache_info.get("size", 0) if cache_info else 0
        )
    
    # è®¡ç®—åŸºç¡€æŒ‡æ ‡
    confidences = [r.get("metadata", {}).get("confidence_score", 0) for r in results]
    similarities = [r.get("metadata", {}).get("similarity_score", 0) for r in results]
    
    avg_confidence = sum(confidences) / len(confidences)
    avg_similarity = sum(similarities) / len(similarities)
    
    # è®¡ç®—åˆ†å¸ƒ
    confidence_dist = {"high": 0, "medium": 0, "low": 0}
    for conf in confidences:
        if conf >= 0.8:
            confidence_dist["high"] += 1
        elif conf >= 0.5:
            confidence_dist["medium"] += 1
        else:
            confidence_dist["low"] += 1
    
    compatibility_dist = {}
    for result in results:
        comp_level = result.get("metadata", {}).get("compatibility_level", "unknown")
        compatibility_dist[comp_level] = compatibility_dist.get(comp_level, 0) + 1
    
    # è®¡ç®—æˆåŠŸç‡
    success_rates = [
        r.get("metadata", {}).get("execution_result", {}).get("success_rate", 0)
        for r in results
    ]
    success_rate = sum(success_rates) / len(success_rates) if success_rates else 0
    
    # è®¡ç®—ååé‡
    throughput = len(results) / retrieval_time if retrieval_time > 0 else 0
    
    return RetrievalMetrics(
        total_results=len(results),
        retrieval_time=retrieval_time,
        cache_hit_rate=cache_info.get("hit_rate", 0) if cache_info else 0,
        avg_confidence=avg_confidence,
        avg_similarity=avg_similarity,
        success_rate=success_rate,
        confidence_distribution=confidence_dist,
        compatibility_distribution=compatibility_dist,
        throughput=throughput,
        memory_usage=0,  # éœ€è¦å®é™…æµ‹é‡
        cache_size=cache_info.get("size", 0) if cache_info else 0
    )
```

### 3.4.5 ä½¿ç”¨ç¤ºä¾‹

```python
# å®Œæ•´çš„ä½¿ç”¨ç¤ºä¾‹
async def example_usage():
    """ä½¿ç”¨ç¤ºä¾‹"""
    
    # 1. é€‰æ‹©äº§å“çº¿æ¡£æ¡ˆ
    profile = TIGER_LAKE_PROFILE
    
    # 2. é€‰æ‹©æ£€ç´¢ç­–ç•¥
    strategy = BALANCED_RETRIEVAL
    
    # 3. åˆ›å»ºæ£€ç´¢å¼•æ“
    engine = KnowledgeRetrievalEngine(profile, strategy)
    
    # 4. æ‰§è¡Œæ£€ç´¢
    query = "PCIeè®¾å¤‡åˆå§‹åŒ–é—®é¢˜"
    additional_filters = {
        "status": "success",
        "min_confidence": 0.8
    }
    
    results = await engine.retrieve_knowledge(query, additional_filters)
    
    # 5. åˆ†æç»“æœ
    print(f"æ£€ç´¢åˆ° {len(results)} ä¸ªç›¸å…³çŸ¥è¯†å•å…ƒ")
    
    for i, result in enumerate(results[:5]):  # æ˜¾ç¤ºå‰5ä¸ªç»“æœ
        metadata = result.get("metadata", {})
        print(f"\nç»“æœ {i+1}:")
        print(f"  ID: {result.get('id')}")
        print(f"  æ ‡é¢˜: {result.get('content', {}).get('title', 'N/A')}")
        print(f"  ç½®ä¿¡åº¦: {metadata.get('confidence_score', 0):.3f}")
        print(f"  ç›¸ä¼¼åº¦: {metadata.get('similarity_score', 0):.3f}")
        print(f"  å…¼å®¹æ€§: {metadata.get('compatibility_level', 'N/A')}")
        print(f"  åŒ¹é…æ ‡ç­¾: {metadata.get('matched_tags', {})}")
        
        # æ˜¾ç¤ºç›¸å…³æ€§è¯„åˆ†
        relevance_score = (
            metadata.get('confidence_score', 0) * 0.6 +
            metadata.get('similarity_score', 0) * 0.4
        )
        print(f"  ç»¼åˆç›¸å…³æ€§: {relevance_score:.3f}")
    
    # 6. è®¡ç®—æ€§èƒ½æŒ‡æ ‡
    metrics = calculate_retrieval_metrics(results, 0.5)  # å‡è®¾æ£€ç´¢è€—æ—¶0.5ç§’
    print(f"\næ£€ç´¢æ€§èƒ½æŒ‡æ ‡:")
    print(f"  æ€»ç»“æœæ•°: {metrics.total_results}")
    print(f"  å¹³å‡ç½®ä¿¡åº¦: {metrics.avg_confidence:.3f}")
    print(f"  å¹³å‡ç›¸ä¼¼åº¦: {metrics.avg_similarity:.3f}")
    print(f"  æˆåŠŸç‡: {metrics.success_rate:.3f}")
    print(f"  ååé‡: {metrics.throughput:.2f} ç»“æœ/ç§’")
```

### 3.4.6 å‘é‡æ£€ç´¢ä¸äº§å“çº¿æƒé‡èåˆå…¬å¼

åœ¨å®é™…æ£€ç´¢è¿‡ç¨‹ä¸­ï¼Œéœ€è¦å°†å‘é‡ç›¸ä¼¼åº¦ä¸äº§å“çº¿åŒ¹é…åº¦è¿›è¡Œèåˆï¼Œä»¥è·å¾—æœ€ç»ˆçš„ç»¼åˆç›¸å…³æ€§è¯„åˆ†ã€‚

#### 3.4.6.1 èåˆå…¬å¼å®šä¹‰

```python
import numpy as np
from typing import Dict, List, Tuple

class ScoreFusionEngine:
    """åˆ†æ•°èåˆå¼•æ“"""
    
    # èåˆæƒé‡é…ç½®
    VECTOR_WEIGHT = 0.6       # å‘é‡ç›¸ä¼¼åº¦æƒé‡
    PRODUCT_LINE_WEIGHT = 0.4  # äº§å“çº¿åŒ¹é…åº¦æƒé‡
    
    @classmethod
    def fuse_scores(
        cls,
        vector_similarity: float,
        product_line_score: float,
        compatibility_level: str = "exact",
        priority_boost: float = 1.0
    ) -> float:
        """
        èåˆå‘é‡ç›¸ä¼¼åº¦ä¸äº§å“çº¿åŒ¹é…åº¦
        
        å…¬å¼: FusedScore = Î± Ã— VS Ã— PB + Î² Ã— PLS Ã— PB
        
        å…¶ä¸­:
        - VS (Vector Similarity): å‘é‡ç›¸ä¼¼åº¦ [0, 1]
        - PLS (Product Line Score): äº§å“çº¿åŒ¹é…åº¦ [0, 1]
        - Î± = 0.6: å‘é‡æƒé‡
        - Î² = 0.4: äº§å“çº¿æƒé‡
        - PB (Priority Boost): ä¼˜å…ˆçº§æå‡ç³»æ•°
        
        å…¼å®¹æ€§çº§åˆ«è°ƒæ•´ç³»æ•°:
        - EXACT: 1.0 (æ— è°ƒæ•´)
        - FAMILY: 0.9
        - ARCH: 0.8
        - GENERIC: 0.6
        """
        # å…¼å®¹æ€§çº§åˆ«è°ƒæ•´
        compatibility_factor = {
            "exact": 1.0,
            "family": 0.9,
            "architecture": 0.8,
            "generic": 0.6
        }.get(compatibility_level, 0.5)
        
        # è®¡ç®—èåˆåˆ†æ•°
        fused_score = (
            cls.VECTOR_WEIGHT * vector_similarity +
            cls.PRODUCT_LINE_WEIGHT * product_line_score
        ) * compatibility_factor * priority_boost
        
        # å½’ä¸€åŒ–åˆ° [0, 1]
        return min(1.0, max(0.0, fused_score))
    
    @classmethod
    def calculate_final_ranking(
        cls,
        candidates: List[Dict],
        vector_weight: float = None,
        product_line_weight: float = None
    ) -> List[Dict]:
        """
        è®¡ç®—æœ€ç»ˆæ’å
        
        å¯¹äºæ¯ä¸ªå€™é€‰ç»“æœ:
        1. è·å–å‘é‡ç›¸ä¼¼åº¦ VS
        2. è·å–äº§å“çº¿åŒ¹é…åº¦ PLS
        3. è·å–å…¼å®¹æ€§çº§åˆ« CL
        4. è·å–ä¼˜å…ˆçº§æå‡ PB
        5. è®¡ç®—èåˆåˆ†æ•° FS
        6. æŒ‰ FS é™åºæ’åº
        """
        vector_w = vector_weight if vector_weight is not None else cls.VECTOR_WEIGHT
        product_w = product_line_weight if product_line_weight is not None else cls.PRODUCT_LINE_WEIGHT
        
        for candidate in candidates:
            metadata = candidate.get("metadata", {})
            
            # æå–å„ç»´åº¦åˆ†æ•°
            vector_sim = metadata.get("vector_similarity", 0.5)
            product_line_score = metadata.get("similarity_score", 0.5)
            compatibility = metadata.get("compatibility_level", "generic")
            priority = metadata.get("priority", "medium")
            
            # ä¼˜å…ˆçº§æå‡
            priority_boost = {
                "critical": 1.3,
                "high": 1.15,
                "medium": 1.0,
                "low": 0.85
            }.get(priority, 1.0)
            
            # è®¡ç®—èåˆåˆ†æ•°
            fused_score = cls.fuse_scores(
                vector_similarity=vector_sim,
                product_line_score=product_line_score,
                compatibility_level=compatibility,
                priority_boost=priority_boost
            )
            
            candidate["metadata"]["fused_score"] = fused_score
            candidate["metadata"]["ranking_factors"] = {
                "vector_similarity": vector_sim,
                "product_line_score": product_line_score,
                "compatibility_level": compatibility,
                "priority_boost": priority_boost,
                "weights": {
                    "vector": vector_w,
                    "product_line": product_w
                }
            }
        
        # æŒ‰èåˆåˆ†æ•°æ’åº
        candidates.sort(
            key=lambda x: x["metadata"].get("fused_score", 0),
            reverse=True
        )
        
        return candidates
```

#### 3.4.6.2 æƒé‡é…ç½®ç¤ºä¾‹

```python
# åœºæ™¯1: é«˜ç²¾åº¦éœ€æ±‚ - æ›´ä¾èµ–å‘é‡æ£€ç´¢
HIGH_PRECISION_WEIGHTS = {
    "vector_weight": 0.8,
    "product_line_weight": 0.2
}

# åœºæ™¯2: å¹³è¡¡æ¨¡å¼ - å‘é‡ä¸äº§å“çº¿åŒç­‰é‡è¦
BALANCED_WEIGHTS = {
    "vector_weight": 0.6,
    "product_line_weight": 0.4
}

# åœºæ™¯3: é«˜å¯ä¿¡åº¦äº§å“çº¿ä¼˜å…ˆ - æ›´ä¾èµ–äº§å“çº¿åŒ¹é…
HIGH_CONFIDENCE_WEIGHTS = {
    "vector_weight": 0.4,
    "product_line_weight": 0.6
}

# åœºæ™¯4: è·¨äº§å“çº¿æ¢ç´¢ - é™ä½äº§å“çº¿æƒé‡
EXPLORATION_WEIGHTS = {
    "vector_weight": 0.7,
    "product_line_weight": 0.3
}

def get_weights_for_scenario(scenario: str) -> Dict[str, float]:
    """æ ¹æ®åœºæ™¯è·å–æƒé‡é…ç½®"""
    scenarios = {
        "high_precision": HIGH_PRECISION_WEIGHTS,
        "balanced": BALANCED_WEIGHTS,
        "high_confidence": HIGH_CONFIDENCE_WEIGHTS,
        "exploration": EXPLORATION_WEIGHTS
    }
    return scenarios.get(scenario, BALANCED_WEIGHTS)
```

### 3.4.7 å¤šçº§å›é€€ç­–ç•¥è¯¦è§£

å½“ç‰¹å®šäº§å“çº¿æ²¡æœ‰æ£€ç´¢ç»“æœæ—¶ï¼Œç³»ç»Ÿé‡‡ç”¨å¤šçº§å›é€€ç­–ç•¥é€æ­¥æ‰©å¤§æ£€ç´¢èŒƒå›´ã€‚

#### 3.4.7.1 å›é€€ç­–ç•¥æµç¨‹

```python
from enum import Enum
from typing import List, Optional, Callable

class FallbackLevel(Enum):
    """å›é€€çº§åˆ«"""
    LEVEL_0_EXACT = 0  # ç²¾ç¡®åŒ¹é…
    LEVEL_1_SOC_FAMILY = 1  # åŒç³»åˆ—SoC
    LEVEL_2_CHIPSET = 2  # ç›¸åŒChipset
    LEVEL_3_FIRMWARE = 3  # ç›¸åŒå›ºä»¶æ ˆ
    LEVEL_4_ARCH = 4  # åŒæ¶æ„
    LEVEL_5_GENERIC = 5  # é€šç”¨è§£å†³æ–¹æ¡ˆ

class MultiLevelFallbackStrategy:
    """å¤šçº§å›é€€ç­–ç•¥"""
    
    # å›é€€è¶…æ—¶é…ç½®ï¼ˆç§’ï¼‰
    TIMEOUTS = {
        FallbackLevel.LEVEL_0_EXACT: 5,
        FallbackLevel.LEVEL_1_SOC_FAMILY: 3,
        FallbackLevel.LEVEL_2_CHIPSET: 2,
        FallbackLevel.LEVEL_3_FIRMWARE: 2,
        FallbackLevel.LEVEL_4_ARCH: 1,
        FallbackLevel.LEVEL_5_GENERIC: 1
    }
    
    # å›é€€é—´éš”ï¼ˆç§’ï¼‰
    INTERVALS = {
        FallbackLevel.LEVEL_0_EXACT: 0,
        FallbackLevel.LEVEL_1_SOC_FAMILY: 0.5,
        FallbackLevel.LEVEL_2_CHIPSET: 0.3,
        FallbackLevel.LEVEL_3_FIRMWARE: 0.3,
        FallbackLevel.LEVEL_4_ARCH: 0.2,
        FallbackLevel.LEVEL_5_GENERIC: 0.2
    }
    
    def __init__(self, max_total_time: float = 15.0):
        self.max_total_time = max_total_time
        self.fallback_chain: List[FallbackLevel] = [
            FallbackLevel.LEVEL_0_EXACT,
            FallbackLevel.LEVEL_1_SOC_FAMILY,
            FallbackLevel.LEVEL_2_CHIPSET,
            FallbackLevel.LEVEL_3_FIRMWARE,
            FallbackLevel.LEVEL_4_ARCH,
            FallbackLevel.LEVEL_5_GENERIC
        ]
    
    def execute_fallback_search(
        self,
        query: str,
        profile: "ProductLineProfile",
        initial_search_func: Callable,
        fallback_search_func: Callable
    ) -> List[Dict]:
        """
        æ‰§è¡Œå›é€€æœç´¢
        
        æµç¨‹:
        1. ç²¾ç¡®åŒ¹é…æœç´¢ â†’ æœ‰ç»“æœåˆ™è¿”å›
        2. Level 1 å›é€€ â†’ æœ‰ç»“æœåˆ™è¿”å›
        3. Level 2 å›é€€ â†’ æœ‰ç»“æœåˆ™è¿”å›
        4. ...
        5. å…¨éƒ¨å›é€€ä»æ— ç»“æœ â†’ è¿”å›ç©ºåˆ—è¡¨
        """
        results = []
        accumulated_time = 0.0
        
        for level in self.fallback_chain:
            # æ£€æŸ¥è¶…æ—¶
            if accumulated_time >= self.max_total_time:
                print(f"â° å›é€€æœç´¢è¶…æ—¶ï¼Œåœç•™åœ¨ Level {level.value}")
                break
            
            level_timeout = self.TIMEOUTS.get(level, 1.0)
            
            # æ‰§è¡Œè¯¥çº§åˆ«çš„æœç´¢
            level_results = self._search_at_level(
                query=query,
                level=level,
                profile=profile,
                search_func=fallback_search_func,
                timeout=level_timeout
            )
            
            accumulated_time += level_timeout + self.INTERVALS.get(level, 0)
            
            if level_results:
                # æ‰¾åˆ°ç»“æœï¼Œæ·»åŠ å…ƒæ•°æ®åè¿”å›
                for result in level_results:
                    result["metadata"]["fallback_level"] = level.value
                    result["metadata"]["fallback_reason"] = self._get_fallback_reason(level)
                
                # åˆå¹¶åˆ°æœ€ç»ˆç»“æœï¼ˆä¿ç•™ä¸åŒçº§åˆ«çš„ç»“æœï¼‰
                results.extend(level_results)
                
                # å¦‚æœæ˜¯ç²¾ç¡®åŒ¹é…çº§åˆ«ï¼Œç›´æ¥è¿”å›
                if level == FallbackLevel.LEVEL_0_EXACT:
                    return results
        
        # æŒ‰èåˆåˆ†æ•°é‡æ–°æ’åº
        if results:
            fusion_engine = ScoreFusionEngine()
            results = fusion_engine.calculate_final_ranking(results)
        
        return results
    
    def _search_at_level(
        self,
        query: str,
        level: FallbackLevel,
        profile: "ProductLineProfile",
        search_func: Callable,
        timeout: float
    ) -> List[Dict]:
        """åœ¨æŒ‡å®šçº§åˆ«æ‰§è¡Œæœç´¢"""
        print(f"ğŸ” æ‰§è¡Œ Level {level.value} ({level.name}) æœç´¢...")
        
        # æ„å»ºè¯¥çº§åˆ«çš„è¿‡æ»¤æ¡ä»¶
        filters = self._build_level_filters(level, profile)
        
        # æ‰§è¡Œæœç´¢ï¼ˆæ¨¡æ‹Ÿè¶…æ—¶æ§åˆ¶ï¼‰
        try:
            results = search_func(query, filters, timeout=timeout)
            print(f"   â†’ Level {level.value} è¿”å› {len(results)} ä¸ªç»“æœ")
            return results
        except Exception as e:
            print(f"   â†’ Level {level.value} æœç´¢å¤±è´¥: {e}")
            return []
    
    def _build_level_filters(
        self,
        level: FallbackLevel,
        profile: "ProductLineProfile"
    ) -> Dict:
        """æ„å»ºå„çº§åˆ«çš„è¿‡æ»¤æ¡ä»¶"""
        filters = {}
        
        if level == FallbackLevel.LEVEL_0_EXACT:
            # ç²¾ç¡®åŒ¹é…
            filters = {
                "soc_type": profile.product_line_tags.get("soc_type"),
                "firmware_stack": profile.product_line_tags.get("firmware_stack"),
                "chipset": profile.product_line_tags.get("chipset") if isinstance(
                    profile.product_line_tags.get("chipset"), str
                ) else None,
                "platform": profile.product_line_tags.get("platform")
            }
        
        elif level == FallbackLevel.LEVEL_1_SOC_FAMILY:
            # åŒç³»åˆ—SoCï¼ˆç§»é™¤chipseté™åˆ¶ï¼‰
            filters = {
                "soc_type": profile.product_line_tags.get("soc_type"),
                "firmware_stack": profile.product_line_tags.get("firmware_stack"),
                "platform": profile.product_line_tags.get("platform")
            }
        
        elif level == FallbackLevel.LEVEL_2_CHIPSET:
            # ç›¸åŒChipsetï¼ˆæ”¾å®½firmware_stackï¼‰
            filters = {
                "soc_type": profile.product_line_tags.get("soc_type"),
                "firmware_stack": None,  # ä¸é™åˆ¶
                "chipset_family": self._get_chipset_family(
                    profile.product_line_tags.get("chipset")
                )
            }
        
        elif level == FallbackLevel.LEVEL_3_FIRMWARE:
            # ç›¸åŒå›ºä»¶æ ˆï¼ˆæ”¾å®½SoCï¼‰
            filters = {
                "firmware_stack": profile.product_line_tags.get("firmware_stack"),
                "platform": profile.product_line_tags.get("platform")
            }
        
        elif level == FallbackLevel.LEVEL_4_ARCH:
            # åŒæ¶æ„
            soc_type = profile.product_line_tags.get("soc_type")
            arch = self._infer_architecture(soc_type)
            filters = {
                "architecture": arch,
                "platform": profile.product_line_tags.get("platform")
            }
        
        elif level == FallbackLevel.LEVEL_5_GENERIC:
            # é€šç”¨è§£å†³æ–¹æ¡ˆï¼ˆä»…ä¿ç•™åŸºç¡€è¿‡æ»¤ï¼‰
            filters = {
                "priority": ["high", "medium"],
                "min_confidence": 0.7
            }
        
        return {k: v for k, v in filters.items() if v is not None}
    
    def _get_chipset_family(self, chipset: str) -> str:
        """è·å–Chipsetå®¶æ—ç³»åˆ—"""
        if not chipset:
            return None
        
        # ç®€åŒ–çš„chipsetå®¶æ—æ˜ å°„
        if chipset.startswith("HM"):
            return "HM_series"
        elif chipset.startswith("WM"):
            return "WM_series"
        elif chipset.startswith("TR"):
            return "TR_series"
        elif chipset.startswith("X"):
            return "X_series"
        else:
            return "other"
    
    def _infer_architecture(self, soc_type: str) -> str:
        """æ¨æ–­æ¶æ„"""
        if not soc_type:
            return "x86_64"
        
        if soc_type.startswith("Tiger") or soc_type.startswith("Alder") or \
           soc_type.startswith("Raptor") or soc_type.startswith("Meteor") or \
           soc_type.startswith("Arrow") or soc_type.startswith("Ice") or \
           soc_type.startswith("Coffee") or soc_type.startswith("Kaby") or \
           soc_type.startswith("Broadwell") or soc_type.startswith("Haswell"):
            return "x86_64"
        elif soc_type.startswith("EPYC") or soc_type.startswith("Ryzen"):
            return "x86_64"
        elif soc_type.startswith("Cortex") or soc_type.startswith("Neoverse"):
            return "AArch64"
        elif soc_type.startswith("Snapdragon"):
            return "AArch64"
        else:
            return "x86_64"
    
    def _get_fallback_reason(self, level: FallbackLevel) -> str:
        """è·å–å›é€€åŸå› æè¿°"""
        reasons = {
            FallbackLevel.LEVEL_0_EXACT: "ç²¾ç¡®åŒ¹é…",
            FallbackLevel.LEVEL_1_SOC_FAMILY: "åŒç³»åˆ—SoCå›é€€",
            FallbackLevel.LEVEL_2_CHIPSET: "ç›¸åŒChipsetå›é€€",
            FallbackLevel.LEVEL_3_FIRMWARE: "ç›¸åŒå›ºä»¶æ ˆå›é€€",
            FallbackLevel.LEVEL_4_ARCH: "åŒæ¶æ„å›é€€",
            FallbackLevel.LEVEL_5_GENERIC: "é€šç”¨è§£å†³æ–¹æ¡ˆå›é€€"
        }
        return reasons.get(level, "æœªçŸ¥å›é€€")
```

#### 3.4.7.2 å›é€€ç­–ç•¥é…ç½®

```python
# å›é€€ç­–ç•¥é…ç½®ç±»
@dataclass
class FallbackConfig:
    """å›é€€ç­–ç•¥é…ç½®"""
    enable_multi_level_fallback: bool = True
    max_fallback_levels: int = 6
    max_total_time_seconds: float = 15.0
    
    # å„çº§åˆ«æ˜¯å¦å¯ç”¨
    enable_exact_match: bool = True
    enable_soc_family: bool = True
    enable_chipset: bool = True
    enable_firmware: bool = True
    enable_arch: bool = True
    enable_generic: bool = True
    
    # ç»“æœåˆå¹¶ç­–ç•¥
    merge_results_from_all_levels: bool = True  # True: åˆå¹¶æ‰€æœ‰çº§åˆ«ç»“æœ, False: åªè¿”å›æœ€é«˜çº§åˆ«çš„ç»“æœ
    
    # åˆ†æ•°è°ƒæ•´
    apply_level_penalty: bool = True
    level_penalty_factor: float = 0.1  # æ¯é™ä¸€çº§æ‰£å‡çš„åˆ†æ•°
    
    def get_active_fallback_levels(self) -> List[FallbackLevel]:
        """è·å–å¯ç”¨çš„å›é€€çº§åˆ«"""
        levels = []
        if self.enable_exact_match:
            levels.append(FallbackLevel.LEVEL_0_EXACT)
        if self.enable_soc_family:
            levels.append(FallbackLevel.LEVEL_1_SOC_FAMILY)
        if self.enable_chipset:
            levels.append(FallbackLevel.LEVEL_2_CHIPSET)
        if self.enable_firmware:
            levels.append(FallbackLevel.LEVEL_3_FIRMWARE)
        if self.enable_arch:
            levels.append(FallbackLevel.LEVEL_4_ARCH)
        if self.enable_generic:
            levels.append(FallbackLevel.LEVEL_5_GENERIC)
        return levels[:self.max_fallback_levels]

# é¢„å®šä¹‰å›é€€ç­–ç•¥é…ç½®
AGGRESSIVE_FALLBACK = FallbackConfig(
    enable_multi_level_fallback=True,
    max_fallback_levels=6,
    max_total_time_seconds=20.0,
    merge_results_from_all_levels=True,
    apply_level_penalty=True,
    level_penalty_factor=0.15
)

BALANCED_FALLBACK = FallbackConfig(
    enable_multi_level_fallback=True,
    max_fallback_levels=4,
    max_total_time_seconds=10.0,
    merge_results_from_all_levels=False,
    apply_level_penalty=True,
    level_penalty_factor=0.1
)

CONSERVATIVE_FALLBACK = FallbackConfig(
    enable_multi_level_fallback=True,
    max_fallback_levels=2,
    max_total_time_seconds=5.0,
    merge_results_from_all_levels=False,
    apply_level_penalty=False
)
```

### 3.4.8 ç¤ºä¾‹åœºæ™¯

#### 3.4.8.1 åœºæ™¯ä¸€ï¼šTiger Lake + UEFI æ£€ç´¢æµç¨‹

```python
"""
åœºæ™¯æè¿°:
- å½“å‰ä»»åŠ¡: æ’æŸ¥ Tiger Lake å¹³å° UEFI å›ºä»¶ä¸‹çš„ PCIe åˆå§‹åŒ–é—®é¢˜
- äº§å“çº¿ä¿¡æ¯:
  - soc_type: Tiger_Lake
  - firmware_stack: UEFI_2.8
  - chipset: HM570
  - platform: Server
- æŸ¥è¯¢æ–‡æœ¬: "PCIe device enumeration timing"
"""

# æ­¥éª¤1: æ„å»ºäº§å“çº¿æ¡£æ¡ˆ
tiger_lake_uefi_profile = ProductLineProfile(
    profile_id="tiger_lake_uefi_server",
    product_line_tags={
        "soc_type": "Tiger_Lake",
        "firmware_stack": "UEFI_2.8",
        "chipset": ["HM570", "WM590"],
        "platform": "Server"
    },
    retrieval_priority=9,
    weight_multipliers={
        "soc_type": 1.0,      # æœ€é«˜æƒé‡
        "firmware_stack": 0.95,
        "chipset": 0.85,
        "platform": 0.7
    },
    compatibility_matrix={
        "Tiger_Lake": CompatibilityLevel.EXACT,
        "Alder_Lake": CompatibilityLevel.FAMILY,
        "Generic_x86_64": CompatibilityLevel.ARCH,
        "UEFI_2.8": CompatibilityLevel.EXACT,
        "UEFI_2.9": CompatibilityLevel.FAMILY,
        "EDK2": CompatibilityLevel.ARCH
    }
)

# æ­¥éª¤2: é…ç½®æ£€ç´¢ç­–ç•¥
strategy = RetrievalStrategy(
    strategy_id="tiger_lake_pcie_search",
    mode=RetrievalMode.BALANCED,
    cache_strategy=CacheStrategy.HYBRID,
    max_results=30,
    confidence_threshold=0.75,
    similarity_threshold=0.65,
    min_success_rate=0.8
)

# æ­¥éª¤3: é…ç½®å›é€€ç­–ç•¥
fallback_config = BALANCED_FALLBACK

# æ­¥éª¤4: æ‰§è¡Œæ£€ç´¢ï¼ˆæ¨¡æ‹Ÿï¼‰
async def simulate_tiger_lake_retrieval():
    """æ¨¡æ‹Ÿ Tiger Lake æ£€ç´¢æµç¨‹"""
    
    query = "PCIe device enumeration timing"
    
    # å‡è®¾æ£€ç´¢ç»“æœ
    mock_results = [
        {
            "id": "ku_20241227_001",
            "content": {
                "title": "PCIeåˆå§‹åŒ–æ—¶åºä¼˜åŒ–",
                "summary": "é’ˆå¯¹Intel Tiger Lakeå¹³å°ï¼ŒPCIeè®¾å¤‡åœ¨å†·å¯åŠ¨åæœªèƒ½æ­£ç¡®æšä¸¾çš„è§£å†³æ–¹æ¡ˆ"
            },
            "metadata": {
                "product_line": {
                    "soc_type": "Tiger_Lake",
                    "firmware_stack": "UEFI_2.8",
                    "chipset": "HM570",
                    "platform": "Server"
                },
                "confidence_score": 0.92,
                "priority": "high"
            }
        },
        {
            "id": "ku_20241226_015",
            "content": {
                "title": "UEFIç¯å¢ƒä¸‹PCIeæšä¸¾è¶…æ—¶å¤„ç†",
                "summary": "åœ¨UEFI 2.8å›ºä»¶ä¸‹å¤„ç†PCIeè®¾å¤‡æšä¸¾è¶…æ—¶çš„æŠ€æœ¯æ–¹æ¡ˆ"
            },
            "metadata": {
                "product_line": {
                    "soc_type": "Tiger_Lake",
                    "firmware_stack": "UEFI_2.8",
                    "chipset": "WM590",
                    "platform": "Server"
                },
                "confidence_score": 0.88,
                "priority": "medium"
            }
        },
        {
            "id": "ku_20241225_032",
            "content": {
                "title": "Alder Lakeå¹³å°PCIeæ—¶åºè°ƒæ•´",
                "summary": "Alder Lakeå¹³å°çš„PCIeåˆå§‹åŒ–æ—¶åºè°ƒæ•´å»ºè®®"
            },
            "metadata": {
                "product_line": {
                    "soc_type": "Alder_Lake",
                    "firmware_stack": "UEFI_2.8",
                    "chipset": "Z590",
                    "platform": "Desktop"
                },
                "confidence_score": 0.85,
                "priority": "medium"
            }
        },
        {
            "id": "ku_20241224_008",
            "content": {
                "title": "x86_64å¹³å°é€šç”¨PCIeåˆå§‹åŒ–ä»£ç ",
                "summary": "é€‚ç”¨äºx86_64æ¶æ„å¹³å°çš„é€šç”¨PCIeåˆå§‹åŒ–è§£å†³æ–¹æ¡ˆ"
            },
            "metadata": {
                "product_line": {
                    "soc_type": "Generic_x86_64",
                    "firmware_stack": "Generic",
                    "chipset": "Generic",
                    "platform": "Generic"
                },
                "confidence_score": 0.72,
                "priority": "low"
            }
        }
    ]
    
    # å‡è®¾çš„å‘é‡ç›¸ä¼¼åº¦
    vector_similarities = [0.88, 0.82, 0.75, 0.65]
    
    # è®¡ç®—äº§å“çº¿åŒ¹é…åº¦
    matcher = ProductLineMatcher(tiger_lake_uefi_profile)
    fusion_engine = ScoreFusionEngine()
    
    print("=" * 60)
    print("Tiger Lake + UEFI æ£€ç´¢æµç¨‹ç¤ºä¾‹")
    print("=" * 60)
    print(f"\næŸ¥è¯¢: {query}")
    print(f"äº§å“çº¿é…ç½®:")
    print(f"  - SoC: Tiger_Lake")
    print(f"  - Firmware: UEFI_2.8")
    print(f"  - Chipset: HM570/WM590")
    print(f"  - Platform: Server")
    print()
    
    # é€ä¸ªå¤„ç†ç»“æœ
    for i, (result, vec_sim) in enumerate(zip(mock_results, vector_similarities)):
        metadata = result["metadata"]
        product_line = metadata.get("product_line", {})
        
        # äº§å“çº¿åŒ¹é…
        match_result = matcher.match_knowledge_unit(result)
        
        # æ¨¡æ‹Ÿå‘é‡ç›¸ä¼¼åº¦
        metadata["vector_similarity"] = vec_sim
        metadata["similarity_score"] = match_result.similarity_score
        metadata["compatibility_level"] = match_result.compatibility_level.value
        metadata["matched_tags"] = match_result.matched_tags
        
        # è®¡ç®—èåˆåˆ†æ•°
        fused_score = fusion_engine.fuse_scores(
            vector_similarity=vec_sim,
            product_line_score=match_result.similarity_score,
            compatibility_level=match_result.compatibility_level.value,
            priority_boost=1.0
        )
        
        metadata["fused_score"] = fused_score
        
        print(f"ç»“æœ {i+1}: {result['content']['title']}")
        print(f"  - äº§å“çº¿: {product_line}")
        print(f"  - å‘é‡ç›¸ä¼¼åº¦: {vec_sim:.3f}")
        print(f"  - äº§å“çº¿åŒ¹é…åº¦: {match_result.similarity_score:.3f}")
        print(f"  - å…¼å®¹æ€§çº§åˆ«: {match_result.compatibility_level.value}")
        print(f"  - åŒ¹é…æ ‡ç­¾: {match_result.matched_tags}")
        print(f"  - èåˆåˆ†æ•°: {fused_score:.3f}")
        print()
    
    # æœ€ç»ˆæ’åº
    ranked_results = fusion_engine.calculate_final_ranking(mock_results)
    
    print("=" * 60)
    print("æœ€ç»ˆæ’åç»“æœ:")
    print("=" * 60)
    for i, result in enumerate(ranked_results):
        print(f"{i+1}. [{result['metadata'].get('fused_score', 0):.3f}] "
              f"{result['content']['title']}")
    
    return ranked_results

# æ‰§è¡Œç¤ºä¾‹
# await simulate_tiger_lake_retrieval()

"""
é¢„æœŸè¾“å‡º:
============================================================
Tiger Lake + UEFI æ£€ç´¢æµç¨‹ç¤ºä¾‹
============================================================

æŸ¥è¯¢: PCIe device enumeration timing
äº§å“çº¿é…ç½®:
  - SoC: Tiger_Lake
  - Firmware: UEFI_2.8
  - Chipset: HM570/WM590
  - Platform: Server

ç»“æœ 1: PCIeåˆå§‹åŒ–æ—¶åºä¼˜åŒ–
  - äº§å“çº¿: {'soc_type': 'Tiger_Lake', 'firmware_stack': 'UEFI_2.8', 'chipset': 'HM570', 'platform': 'Server'}
  - å‘é‡ç›¸ä¼¼åº¦: 0.880
  - äº§å“çº¿åŒ¹é…åº¦: 1.000
  - å…¼å®¹æ€§çº§åˆ«: exact
  - åŒ¹é…æ ‡ç­¾: {'soc_type': 'Tiger_Lake', 'firmware_stack': 'UEFI_2.8', 'chipset': 'HM570'}
  - èåˆåˆ†æ•°: 0.928

ç»“æœ 2: UEFIç¯å¢ƒä¸‹PCIeæšä¸¾è¶…æ—¶å¤„ç†
  - äº§å“çº¿: {'soc_type': 'Tiger_Lake', 'firmware_stack': 'UEFI_2.8', 'chipset': 'WM590', 'platform': 'Server'}
  - å‘é‡ç›¸ä¼¼åº¦: 0.820
  - äº§å“çº¿åŒ¹é…åº¦: 0.967
  - å…¼å®¹æ€§çº§åˆ«: exact
  - åŒ¹é…æ ‡ç­¾: {'soc_type': 'Tiger_Lake', 'firmware_stack': 'UEFI_2.8', 'chipset': 'WM590'}
  - èåˆåˆ†æ•°: 0.875

ç»“æœ 3: Alder Lakeå¹³å°PCIeæ—¶åºè°ƒæ•´
  - äº§å“çº¿: {'soc_type': 'Alder_Lake', 'firmware_stack': 'UEFI_2.8', 'chipset': 'Z590', 'platform': 'Desktop'}
  - å‘é‡ç›¸ä¼¼åº¦: 0.750
  - äº§å“çº¿åŒ¹é…åº¦: 0.850
  - å…¼å®¹æ€§çº§åˆ«: family
  åŒ¹é…æ ‡ç­¾: {'soc_type': 'Alder_Lake', 'firmware_stack': 'UEFI_2.8'}
  - èåˆåˆ†æ•°: 0.734

ç»“æœ 4: x86_64å¹³å°é€šç”¨PCIeåˆå§‹åŒ–ä»£ç 
  - äº§å“çº¿: {'soc_type': 'Generic_x86_64', 'firmware_stack': 'Generic', 'chipset': 'Generic', 'platform': 'Generic'}
  - å‘é‡ç›¸ä¼¼åº¦: 0.650
  - äº§å“çº¿åŒ¹é…åº¦: 0.500
  - å…¼å®¹æ€§çº§åˆ«: generic
  - åŒ¹é…æ ‡ç­¾: {}
  - èåˆåˆ†æ•°: 0.414

============================================================
æœ€ç»ˆæ’åç»“æœ:
============================================================
1. [0.928] PCIeåˆå§‹åŒ–æ—¶åºä¼˜åŒ–
2. [0.875] UEFIç¯å¢ƒä¸‹PCIeæšä¸¾è¶…æ—¶å¤„ç†
3. [0.734] Alder Lakeå¹³å°PCIeæ—¶åºè°ƒæ•´
4. [0.414] x86_64å¹³å°é€šç”¨PCIeåˆå§‹åŒ–ä»£ç 
"""
```

#### 3.4.8.2 åœºæ™¯äºŒï¼šè·¨äº§å“çº¿æ£€ç´¢

```python
"""
åœºæ™¯æè¿°:
- å½“å‰ä»»åŠ¡: æ’æŸ¥ Intel x86_64 æ¶æ„å¹³å°çš„ USB æ§åˆ¶å™¨é—®é¢˜ï¼Œä½†ä¸ç¡®å®šå…·ä½“çš„ SoC å‹å·
- äº§å“çº¿ä¿¡æ¯:
  - soc_type: Generic_x86_64 (å·²çŸ¥æ¶æ„ï¼Œä½†ä¸ç¡®å®šå…·ä½“å‹å·)
  - firmware_stack: UEFI (å·²çŸ¥å›ºä»¶ç±»å‹)
- æŸ¥è¯¢æ–‡æœ¬: "USB controller initialization timeout"
"""

async def cross_product_line_retrieval_example():
    """è·¨äº§å“çº¿æ£€ç´¢ç¤ºä¾‹"""
    
    # åœºæ™¯é…ç½®
    query = "USB controller initialization timeout"
    current_product_line = {
        "soc_type": "Generic_x86_64",
        "firmware_stack": "UEFI",
        "platform": "Server"
    }
    
    # çŸ¥è¯†åº“ä¸­çš„çŸ¥è¯†å•å…ƒï¼ˆä¸åŒäº§å“çº¿ï¼‰
    knowledge_units = [
        # Tiger Lake å¹³å°
        {
            "id": "ku_001",
            "content": {"title": "Tiger Lake USBæ§åˆ¶å™¨åˆå§‹åŒ–ä¼˜åŒ–"},
            "metadata": {
                "product_line": {
                    "soc_type": "Tiger_Lake",
                    "firmware_stack": "UEFI_2.8"
                },
                "confidence_score": 0.95
            }
        },
        # Alder Lake å¹³å°
        {
            "id": "ku_002",
            "content": {"title": "Alder Lake USB 3.0 ç«¯å£é…ç½®"},
            "metadata": {
                "product_line": {
                    "soc_type": "Alder_Lake",
                    "firmware_stack": "UEFI_2.9"
                },
                "confidence_score": 0.90
            }
        },
        # EPYC (AMD) å¹³å°
        {
            "id": "ku_003",
            "content": {"title": "EPYCå¹³å°USBæ§åˆ¶å™¨é©±åŠ¨é€‚é…"},
            "metadata": {
                "product_line": {
                    "soc_type": "EPYC_Milan",
                    "firmware_stack": "UEFI_2.8"
                },
                "confidence_score": 0.88
            }
        },
        # é€šç”¨ x86_64
        {
            "id": "ku_004",
            "content": {"title": "x86_64é€šç”¨USBåˆå§‹åŒ–æµç¨‹"},
            "metadata": {
                "product_line": {
                    "soc_type": "Generic_x86_64",
                    "firmware_stack": "Generic"
                },
                "confidence_score": 0.75
            }
        }
    ]
    
    # å‘é‡ç›¸ä¼¼åº¦ï¼ˆæ¨¡æ‹Ÿï¼‰
    vector_similarities = [0.85, 0.80, 0.78, 0.72]
    
    print("=" * 60)
    print("è·¨äº§å“çº¿æ£€ç´¢ç¤ºä¾‹")
    print("=" * 60)
    print(f"\næŸ¥è¯¢: {query}")
    print(f"å½“å‰äº§å“çº¿: {current_product_line}")
    print()
    
    # é€šç”¨ x86_64 æ¡£æ¡ˆ
    generic_profile = ProductLineProfile(
        profile_id="generic_x86_64_uefi",
        product_line_tags={
            "soc_type": "Generic_x86_64",
            "firmware_stack": "UEFI"
        },
        retrieval_priority=1,
        weight_multipliers={
            "soc_type": 0.6,
            "firmware_stack": 0.8,
            "platform": 0.5
        },
        compatibility_matrix={
            "Generic_x86_64": CompatibilityLevel.EXACT,
            "Tiger_Lake": CompatibilityLevel.ARCH,
            "Alder_Lake": CompatibilityLevel.ARCH,
            "EPYC_Milan": CompatibilityLevel.ARCH,
            "UEFI": CompatibilityLevel.EXACT,
            "UEFI_2.8": CompatibilityLevel.FAMILY,
            "UEFI_2.9": CompatibilityLevel.FAMILY
        }
    )
    
    matcher = ProductLineMatcher(generic_profile)
    fusion_engine = ScoreFusionEngine()
    
    # å¤„ç†æ¯ä¸ªçŸ¥è¯†å•å…ƒ
    for ku, vec_sim in zip(knowledge_units, vector_similarities):
        metadata = ku["metadata"]
        metadata["vector_similarity"] = vec_sim
        
        match_result = matcher.match_knowledge_unit(ku)
        
        metadata["similarity_score"] = match_result.similarity_score
        metadata["compatibility_level"] = match_result.compatibility_level.value
        metadata["matched_tags"] = match_result.matched_tags
        
        fused_score = fusion_engine.fuse_scores(
            vector_similarity=vec_sim,
            product_line_score=match_result.similarity_score,
            compatibility_level=match_result.compatibility_level.value
        )
        
        metadata["fused_score"] = fused_score
        
        print(f"çŸ¥è¯†å•å…ƒ: {ku['content']['title']}")
        print(f"  - åŸå§‹äº§å“çº¿: {ku['metadata']['product_line']}")
        print(f"  - å‘é‡ç›¸ä¼¼åº¦: {vec_sim:.3f}")
        print(f"  - äº§å“çº¿åŒ¹é…åº¦: {match_result.similarity_score:.3f}")
        print(f"  - å…¼å®¹æ€§çº§åˆ«: {match_result.compatibility_level.value}")
        print(f"  - èåˆåˆ†æ•°: {fused_score:.3f}")
        print()
    
    # æ’åºç»“æœ
    ranked = fusion_engine.calculate_final_ranking(knowledge_units)
    
    print("è·¨äº§å“çº¿æ£€ç´¢ç»“æœæ’å:")
    for i, ku in enumerate(ranked):
        print(f"  {i+1}. [{ku['metadata']['fused_score']:.3f}] "
              f"{ku['content']['title']} "
              f"(åŸäº§å“çº¿: {ku['metadata']['product_line']['soc_type']})")

# await cross_product_line_retrieval_example()

"""
é¢„æœŸè¾“å‡º:
============================================================
è·¨äº§å“çº¿æ£€ç´¢ç¤ºä¾‹
============================================================

æŸ¥è¯¢: USB controller initialization timeout
å½“å‰äº§å“çº¿: {'soc_type': 'Generic_x86_64', 'firmware_stack': 'UEFI', 'platform': 'Server'}

çŸ¥è¯†å•å…ƒ: Tiger Lake USBæ§åˆ¶å™¨åˆå§‹åŒ–ä¼˜åŒ–
  - åŸå§‹äº§å“çº¿: {'soc_type': 'Tiger_Lake', 'firmware_stack': 'UEFI_2.8'}
  - å‘é‡ç›¸ä¼¼åº¦: 0.850
  - äº§å“çº¿åŒ¹é…åº¦: 0.733
  - å…¼å®¹æ€§çº§åˆ«: architecture
  - èåˆåˆ†æ•°: 0.720

çŸ¥è¯†å•å…ƒ: Alder Lake USB 3.0 ç«¯å£é…ç½®
  - åŸå§‹äº§å“çº¿: {'soc_type': 'Alder_Lake', 'firmware_stack': 'UEFI_2.9'}
  - å‘é‡ç›¸ä¼¼åº¦: 0.800
  - äº§å“çº¿åŒ¹é…åº¦: 0.733
  - å…¼å®¹æ€§çº§åˆ«: architecture
  - èåˆåˆ†æ•°: 0.696

çŸ¥è¯†å•å…ƒ: EPYCå¹³å°USBæ§åˆ¶å™¨é©±åŠ¨é€‚é…
  - åŸå§‹äº§å“çº¿: {'soc_type': 'EPYC_Milan', 'firmware_stack': 'UEFI_2.8'}
  - å‘é‡ç›¸ä¼¼åº¦: 0.780
  - äº§å“çº¿åŒ¹é…åº¦: 0.700
  - å…¼å®¹æ€§çº§åˆ«: architecture
  - èåˆåˆ†æ•°: 0.672

çŸ¥è¯†å•å…ƒ: x86_64é€šç”¨USBåˆå§‹åŒ–æµç¨‹
  - åŸå§‹äº§å“çº¿: {'soc_type': 'Generic_x86_64', 'firmware_stack': 'Generic'}
  - å‘é‡ç›¸ä¼¼åº¦: 0.720
  - äº§å“çº¿åŒ¹é…åº¦: 0.700
  - å…¼å®¹æ€§çº§åˆ«: exact
  - èåˆåˆ†æ•°: 0.616

è·¨äº§å“çº¿æ£€ç´¢ç»“æœæ’å:
  1. [0.720] Tiger Lake USBæ§åˆ¶å™¨åˆå§‹åŒ–ä¼˜åŒ– (åŸäº§å“çº¿: Tiger_Lake)
  2. [0.696] Alder Lake USB 3.0 ç«¯å£é…ç½® (åŸäº§å“çº¿: Alder_Lake)
  3. [0.672] EPYCå¹³å°USBæ§åˆ¶å™¨é©±åŠ¨é€‚é… (åŸäº§å“çº¿: EPYC_Milan)
  4. [0.616] x86_64é€šç”¨USBåˆå§‹åŒ–æµç¨‹ (åŸäº§å“çº¿: Generic_x86_64)
"""
```

#### 3.4.8.3 åœºæ™¯ä¸‰ï¼šæƒé‡å‚æ•°å®é™…å–å€¼æŒ‡å—

```python
"""
æƒé‡å‚æ•°é…ç½®æŒ‡å—

ä»¥ä¸‹æä¾›ä¸åŒåœºæ™¯ä¸‹çš„æ¨èæƒé‡é…ç½®å’Œå‚æ•°å–å€¼è¯´æ˜ã€‚
"""

# ============================================================
# 1. äº§å“çº¿æƒé‡é…ç½® (weight_multipliers)
# ============================================================
# 
# æ¨èå–å€¼èŒƒå›´å’Œè¯´æ˜:
# - soc_type: 0.9 ~ 1.0 (SoCç±»å‹æœ€å…³é”®ï¼Œå¿…é¡»é«˜æƒé‡)
# - firmware_stack: 0.8 ~ 0.95 (å›ºä»¶æ ˆå…¼å®¹æ€§å¾ˆé‡è¦)
# - chipset: 0.6 ~ 0.85 (Chipsetæœ‰ä¸€å®šå½±å“ï¼Œä½†ä¸æ˜¯å†³å®šæ€§)
# - platform: 0.5 ~ 0.7 (å¹³å°ç±»å‹å½±å“ç›¸å¯¹è¾ƒå°)
#
# ç¤ºä¾‹é…ç½®:

WEIGHT_CONFIGS = {
    # åœºæ™¯: ä¸¥æ ¼äº§å“çº¿åŒ¹é… (å¦‚: å®‰å…¨å…³é”®ç³»ç»Ÿ)
    "strict": {
        "soc_type": 1.0,
        "firmware_stack": 1.0,
        "chipset": 0.9,
        "platform": 0.8
    },
    
    # åœºæ™¯: æ ‡å‡†æœåŠ¡å™¨éƒ¨ç½² (æ¨èé»˜è®¤é…ç½®)
    "standard_server": {
        "soc_type": 1.0,
        "firmware_stack": 0.95,
        "chipset": 0.85,
        "platform": 0.7
    },
    
    # åœºæ™¯: æ¡Œé¢/åµŒå…¥å¼ç³»ç»Ÿ
    "desktop_embedded": {
        "soc_type": 0.95,
        "firmware_stack": 0.9,
        "chipset": 0.75,
        "platform": 0.8
    },
    
    # åœºæ™¯: è·¨å¹³å°å…¼å®¹æ€§ä¼˜å…ˆ
    "cross_platform": {
        "soc_type": 0.8,
        "firmware_stack": 0.85,
        "chipset": 0.6,
        "platform": 0.9
    },
    
    # åœºæ™¯: æ¶æ„çº§é€šç”¨ (æœ€å°äº§å“çº¿çº¦æŸ)
    "architecture_only": {
        "soc_type": 0.5,
        "firmware_stack": 0.6,
        "chipset": 0.3,
        "platform": 0.4
    }
}

# ============================================================
# 2. æ£€ç´¢æƒé‡é…ç½® (å‘é‡ vs äº§å“çº¿)
# ============================================================
#
# æ¨èå–å€¼èŒƒå›´å’Œè¯´æ˜:
# - VECTOR_WEIGHT: 0.4 ~ 0.7 (è¯­ä¹‰ç›¸ä¼¼åº¦)
# - PRODUCT_LINE_WEIGHT: 0.3 ~ 0.6 (äº§å“çº¿åŒ¹é…åº¦)
# - æ€»å’Œåº”æ¥è¿‘ 1.0
#
# ç¤ºä¾‹é…ç½®:

RETRIEVAL_WEIGHT_CONFIGS = {
    # åœºæ™¯: è¯­ä¹‰ä¼˜å…ˆ (å‘é‡æ£€ç´¢ä¸ºä¸»)
    "semantic_first": {
        "vector_weight": 0.7,
        "product_line_weight": 0.3
    },
    
    # åœºæ™¯: å¹³è¡¡æ¨¡å¼ (æ¨èé»˜è®¤)
    "balanced": {
        "vector_weight": 0.6,
        "product_line_weight": 0.4
    },
    
    # åœºæ™¯: äº§å“çº¿ä¼˜å…ˆ (ç‰¹å®šå¹³å°)
    "product_line_first": {
        "vector_weight": 0.4,
        "product_line_weight": 0.6
    },
    
    # åœºæ™¯: æ¢ç´¢æ¨¡å¼ (æ‰©å¤§æ£€ç´¢èŒƒå›´)
    "exploration": {
        "vector_weight": 0.65,
        "product_line_weight": 0.35
    }
}

# ============================================================
# 3. å…¼å®¹æ€§çº§åˆ«è¯„åˆ† (Compatibility Scores)
# ============================================================
#
# å®šä¹‰ä¸åŒå…¼å®¹æ€§çº§åˆ«çš„é™æƒç³»æ•°:

COMPATIBILITY_SCORES = {
    # å®Œå…¨åŒ¹é… - æ— é™æƒ
    "exact": {
        "score": 1.0,
        "description": "æ‰€æœ‰æ ‡ç­¾å®Œå…¨åŒ¹é…",
        "example": "Tiger_Lake + UEFI_2.8 â†’ Tiger_Lake + UEFI_2.8"
    },
    
    # åŒç³»åˆ— - è½»å¾®é™æƒ (10%)
    "family": {
        "score": 0.9,
        "description": "åŒç³»åˆ—äº§å“å…¼å®¹",
        "example": "Tiger_Lake â†’ Alder_Lake (Intel 12th vs 13th Gen)"
    },
    
    # åŒæ¶æ„ - ä¸­åº¦é™æƒ (20%)
    "architecture": {
        "score": 0.8,
        "description": "åŒæ¶æ„ä½†ä¸åŒå‚å•†",
        "example": "Tiger_Lake (Intel) â†’ EPYC_Milan (AMD)"
    },
    
    # é€šç”¨ - è¾ƒå¤§é™æƒ (40%)
    "generic": {
        "score": 0.6,
        "description": "é€šç”¨è§£å†³æ–¹æ¡ˆ",
        "example": "Tiger_Lake â†’ Generic_x86_64"
    }
}

# ============================================================
# 4. å›é€€ç­–ç•¥è¶…æ—¶é…ç½®
# ============================================================
#
# æ¨èå–å€¼åŸåˆ™:
# - ç²¾ç¡®åŒ¹é…: 5-10ç§’ (ä¸»è¦æ£€ç´¢è·¯å¾„)
# - åŒç³»åˆ—å›é€€: 3-5ç§’
# - Chipsetå›é€€: 2-3ç§’
# - å›ºä»¶å›é€€: 2-3ç§’
# - æ¶æ„å›é€€: 1-2ç§’
# - é€šç”¨å›é€€: 1-2ç§’
#
# æ€»è¶…æ—¶: 15-25ç§’

FALLBACK_TIMEOUT_CONFIGS = {
    # å¿«é€Ÿæ¨¡å¼ (æ€»è¶…æ—¶ 10ç§’)
    "fast": {
        "exact": 4,
        "family": 2,
        "chipset": 1.5,
        "firmware": 1.5,
        "architecture": 1,
        "generic": 1,
        "max_total": 10
    },
    
    # å¹³è¡¡æ¨¡å¼ (æ¨èé»˜è®¤, æ€»è¶…æ—¶ 15ç§’)
    "balanced": {
        "exact": 6,
        "family": 3,
        "chipset": 2,
        "firmware": 2,
        "architecture": 1,
        "generic": 1,
        "max_total": 15
    },
    
    # æ·±åº¦æ¨¡å¼ (æ€»è¶…æ—¶ 25ç§’)
    "deep": {
        "exact": 10,
        "family": 5,
        "chipset": 3,
        "firmware": 3,
        "architecture": 2,
        "generic": 2,
        "max_total": 25
    }
}

# ============================================================
# 5. é˜ˆå€¼é…ç½®å»ºè®®
# ============================================================

THRESHOLD_CONFIGS = {
    # ä¸¥æ ¼åœºæ™¯
    "strict": {
        "confidence_threshold": 0.85,
        "similarity_threshold": 0.75,
        "min_success_rate": 0.9,
        "min_vector_similarity": 0.8
    },
    
    # æ ‡å‡†åœºæ™¯ (æ¨èé»˜è®¤)
    "standard": {
        "confidence_threshold": 0.75,
        "similarity_threshold": 0.65,
        "min_success_rate": 0.8,
        "min_vector_similarity": 0.7
    },
    
    # å®½æ¾åœºæ™¯
    "relaxed": {
        "confidence_threshold": 0.6,
        "similarity_threshold": 0.5,
        "min_success_rate": 0.7,
        "min_vector_similarity": 0.5
    },
    
    # æ¢ç´¢åœºæ™¯
    "exploration": {
        "confidence_threshold": 0.5,
        "similarity_threshold": 0.4,
        "min_success_rate": 0.6,
        "min_vector_similarity": 0.4
    }
}

# ============================================================
# 6. é…ç½®éªŒè¯å‡½æ•°
# ============================================================

def validate_weight_config(config: Dict[str, float], 
                          expected_keys: List[str] = None) -> Tuple[bool, List[str]]:
    """
    éªŒè¯æƒé‡é…ç½®çš„æœ‰æ•ˆæ€§
    
    è¿”å›: (æ˜¯å¦æœ‰æ•ˆ, é”™è¯¯åˆ—è¡¨)
    """
    errors = []
    
    # æ£€æŸ¥å¿…å¡«å­—æ®µ
    required_keys = expected_keys or ["soc_type", "firmware_stack"]
    for key in required_keys:
        if key not in config:
            errors.append(f"ç¼ºå°‘å¿…å¡«å­—æ®µ: {key}")
    
    # æ£€æŸ¥å–å€¼èŒƒå›´
    for key, value in config.items():
        if not 0 <= value <= 1:
            errors.append(f"{key} å–å€¼ {value} è¶…å‡º [0, 1] èŒƒå›´")
    
    # æ£€æŸ¥æ€»å’Œï¼ˆå¦‚æœæ˜¯æ£€ç´¢æƒé‡é…ç½®ï¼‰
    if "vector_weight" in config and "product_line_weight" in config:
        total = config["vector_weight"] + config["product_line_weight"]
        if abs(total - 1.0) > 0.01:  # å…è®¸0.01çš„è¯¯å·®
            errors.append(f"æƒé‡æ€»å’Œ {total} ä¸ç­‰äº 1.0")
    
    return len(errors) == 0, errors

# é…ç½®éªŒè¯ç¤ºä¾‹
# is_valid, errors = validate_weight_config(WEIGHT_CONFIGS["standard_server"])
# if not is_valid:
#     print(f"é…ç½®æ— æ•ˆ: {errors}")
```

### 3.4.9 æ’åºå’Œè¿‡æ»¤è§„åˆ™

```python
from typing import List, Dict, Optional, Callable
from dataclasses import dataclass, field
from enum import Enum
import time

class SortOrder(Enum):
    """æ’åºé¡ºåº"""
    ASC = "asc"
    DESC = "desc"

class FilterAction(Enum):
    """è¿‡æ»¤åŠ¨ä½œ"""
    INCLUDE = "include"
    EXCLUDE = "exclude"

@dataclass
class SortRule:
    """æ’åºè§„åˆ™"""
    field: str
    order: SortOrder = SortOrder.DESC
    weight: float = 1.0

@dataclass
class FilterRule:
    """è¿‡æ»¤è§„åˆ™"""
    field: str
    value: any
    action: FilterAction = FilterAction.INCLUDE
    condition: Optional[Callable] = None

@dataclass
class RankingConfig:
    """æ’åºå’Œè¿‡æ»¤é…ç½®"""
    # æ’åºè§„åˆ™åˆ—è¡¨
    sort_rules: List[SortRule] = field(default_factory=list)
    
    # è¿‡æ»¤è§„åˆ™åˆ—è¡¨
    filter_rules: List[FilterRule] = field(default_factory=list)
    
    # åˆ†æ•°é˜ˆå€¼
    min_fused_score: float = 0.0
    max_results: int = 100
    
    # å»é‡é…ç½®
    enable_deduplication: bool = True
    deduplication_field: str = "id"
    
    # åˆ†æ•°å¹³æ»‘
    enable_score_smoothing: bool = False
    smoothing_factor: float = 0.1

class KnowledgeUnitRanker:
    """çŸ¥è¯†å•å…ƒæ’åºå™¨"""
    
    def __init__(self, config: RankingConfig):
        self.config = config
    
    def apply_filters(self, results: List[Dict]) -> List[Dict]:
        """åº”ç”¨è¿‡æ»¤è§„åˆ™"""
        filtered = results
        
        for rule in self.config.filter_rules:
            filtered = self._apply_single_filter(filtered, rule)
        
        # åˆ†æ•°é˜ˆå€¼è¿‡æ»¤
        if self.config.min_fused_score > 0:
            filtered = [
                r for r in filtered
                if r.get("metadata", {}).get("fused_score", 0) >= self.config.min_fused_score
            ]
        
        return filtered
    
    def _apply_single_filter(self, results: List[Dict], rule: FilterRule) -> List[Dict]:
        """åº”ç”¨å•æ¡è¿‡æ»¤è§„åˆ™"""
        if rule.condition:
            # è‡ªå®šä¹‰è¿‡æ»¤æ¡ä»¶
            if rule.action == FilterAction.INCLUDE:
                return [r for r in results if rule.condition(r)]
            else:
                return [r for r in results if not rule.condition(r)]
        else:
            # å­—æ®µå€¼åŒ¹é…
            def matches(result):
                value = self._get_nested_value(result, rule.field)
                if isinstance(rule.value, list):
                    return value in rule.value
                return value == rule.value
            
            if rule.action == FilterAction.INCLUDE:
                return [r for r in results if matches(r)]
            else:
                return [r for r in results if not matches(r)]
    
    def apply_sorting(self, results: List[Dict]) -> List[Dict]:
        """åº”ç”¨æ’åºè§„åˆ™"""
        if not self.config.sort_rules:
            return results
        
        def sort_key(item):
            scores = []
            for rule in self.config.sort_rules:
                value = self._get_nested_value(item, f"metadata.{rule.field}", 
                                               default=0, check_fused=True)
                if rule.order == SortOrder.DESC:
                    scores.append(-value * rule.weight)
                else:
                    scores.append(value * rule.weight)
            return tuple(scores)
        
        sorted_results = sorted(results, key=sort_key)
        
        # åº”ç”¨åˆ†æ•°å¹³æ»‘ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.config.enable_score_smoothing:
            sorted_results = self._apply_score_smoothing(sorted_results)
        
        return sorted_results
    
    def _apply_score_smoothing(self, results: List[Dict]) -> List[Dict]:
        """åº”ç”¨åˆ†æ•°å¹³æ»‘ï¼Œé¿å…åˆ†æ•°å·®è·è¿‡å¤§"""
        if not results:
            return results
        
        scores = [r.get("metadata", {}).get("fused_score", 0) for r in results]
        max_score = max(scores)
        min_score = min(scores)
        
        if max_score == min_score:
            return results
        
        # ä½¿ç”¨å¯¹æ•°å¹³æ»‘
        for result in results:
            original_score = result.get("metadata", {}).get("fused_score", 0)
            # å½’ä¸€åŒ–åˆ° [0, 1]
            normalized = (original_score - min_score) / (max_score - min_score)
            # å¯¹æ•°å˜æ¢
            smoothed = 0.1 + 0.9 * (normalized ** 0.5)
            result["metadata"]["smoothed_score"] = smoothed
        
        # æŒ‰å¹³æ»‘åˆ†æ•°é‡æ–°æ’åº
        results.sort(
            key=lambda x: x["metadata"].get("smoothed_score", 0),
            reverse=True
        )
        
        return results
    
    def apply_deduplication(self, results: List[Dict]) -> List[Dict]:
        """åº”ç”¨å»é‡"""
        if not self.config.enable_deduplication:
            return results
        
        seen = set()
        unique_results = []
        
        for result in results:
            key = self._get_nested_value(result, self.config.deduplication_field)
            if key and key not in seen:
                seen.add(key)
                unique_results.append(result)
        
        return unique_results
    
    def _get_nested_value(self, obj: Dict, path: str, default=None, check_fused: bool = False) -> any:
        """è·å–åµŒå¥—å­—å…¸çš„å€¼"""
        # æ”¯æŒç‚¹å·åˆ†éš”çš„è·¯å¾„
        keys = path.split(".")
        
        # å¦‚æœæ£€æŸ¥ fused_scoreï¼Œå…ˆå°è¯•ä» metadata è·å–
        if check_fused and len(keys) >= 2 and keys[0] == "metadata" and keys[1] == "fused_score":
            return obj.get("metadata", {}).get("fused_score", default)
        
        current = obj
        for key in keys:
            if isinstance(current, dict):
                current = current.get(key)
            else:
                return default
            if current is None:
                return default
        
        return current
    
    def rank(self, results: List[Dict]) -> List[Dict]:
        """
        å®Œæ•´çš„æ’åºæµç¨‹:
        1. åº”ç”¨è¿‡æ»¤
        2. åº”ç”¨å»é‡
        3. åº”ç”¨æ’åº
        4. æˆªæ–­ç»“æœ
        """
        # åº”ç”¨è¿‡æ»¤
        filtered = self.apply_filters(results)
        
        # åº”ç”¨å»é‡
        deduplicated = self.apply_deduplication(filtered)
        
        # åº”ç”¨æ’åº
        sorted_results = self.apply_sorting(deduplicated)
        
        # æˆªæ–­ç»“æœ
        return sorted_results[:self.config.max_results]

# é¢„å®šä¹‰æ’åºé…ç½®
RANKING_CONFIGS = {
    # é»˜è®¤æ’åº (èåˆåˆ†æ•°é™åº)
    "default": RankingConfig(
        sort_rules=[
            SortRule(field="fused_score", order=SortOrder.DESC),
            SortRule(field="confidence_score", order=SortOrder.DESC)
        ]
    ),
    
    # ç²¾ç¡®åº¦ä¼˜å…ˆ (ç½®ä¿¡åº¦é™åº)
    "confidence_first": RankingConfig(
        sort_rules=[
            SortRule(field="confidence_score", order=SortOrder.DESC, weight=1.5),
            SortRule(field="fused_score", order=SortOrder.DESC)
        ],
        filter_rules=[
            FilterRule(field="execution_result.status", value="success")
        ]
    ),
    
    # æ–°é²œåº¦ä¼˜å…ˆ (æ›´æ–°æ—¶é—´é™åº)
    "freshness_first": RankingConfig(
        sort_rules=[
            SortRule(field="updated_at", order=SortOrder.DESC),
            SortRule(field="fused_score", order=SortOrder.DESC)
        ],
        filter_rules=[
            FilterRule(field="confidence_score", value=0.7, action=FilterAction.INCLUDE)
        ]
    ),
    
    # äº§å“çº¿ç²¾ç¡®åŒ¹é…ä¼˜å…ˆ
    "exact_product_line": RankingConfig(
        sort_rules=[
            SortRule(field="similarity_score", order=SortOrder.DESC, weight=1.5),
            SortRule(field="fused_score", order=SortOrder.DESC)
        ],
        filter_rules=[
            FilterRule(field="compatibility_level", value="exact", 
                      action=FilterAction.INCLUDE)
        ]
    ),
    
    # æˆåŠŸç‡ä¼˜å…ˆ
    "success_rate_first": RankingConfig(
        sort_rules=[
            SortRule(field="execution_result.success_rate", order=SortOrder.DESC, weight=2.0),
            SortRule(field="fused_score", order=SortOrder.DESC)
        ]
    )
}

def get_ranking_config(config_name: str) -> RankingConfig:
    """è·å–é¢„å®šä¹‰æ’åºé…ç½®"""
    return RANKING_CONFIGS.get(config_name, RANKING_CONFIGS["default"])
```

---

## 4. Qdrant å‘é‡æ•°æ®åº“ Schema

### 4.1 Collection é…ç½®

```json
{
  "collection_name": "knowledge_units",
  "vectors": {
    "size": 1536,
    "distance": "Cosine",
    "on_disk_payload": true
  },
  "optimizers_config": {
    "default_segment_number": 2,
    "max_search_threads": 4,
    "indexing_threshold": 20000
  },
  "hnsw_config": {
    "m": 16,
    "ef_construct": 100,
    "full_scan_threshold": 10000
  },
  "wal_config": {
    "wal_capacity_mb": 32,
    "wal_segments_ahead": 4
  }
}
```

### 4.2 Payload Schema å®šä¹‰

```json
{
  "key": "metadata",
  "fields": {
    "product_line": {
      "type": "keyword",
      "fields": {
        "soc_type": {"type": "keyword"},
        "firmware_stack": {"type": "keyword"},
        "chipset": {"type": "keyword"},
        "platform": {"type": "keyword"}
      }
    },
    "test_context": {
      "type": "object",
      "fields": {
        "test_environment": {"type": "keyword"},
        "test_board": {"type": "keyword"},
        "test_duration": {"type": "keyword"}
      }
    },
    "execution_result": {
      "type": "object", 
      "fields": {
        "status": {"type": "keyword"},
        "execution_time": {"type": "datetime"},
        "iterations_count": {"type": "integer"},
        "success_rate": {"type": "float"}
      }
    },
    "tags": {
      "type": "keyword",
      "is_array": true
    },
    "priority": {"type": "keyword"},
    "confidence_score": {"type": "float"}
  }
}
```

### 4.3 åˆ›å»º Collection çš„ Python ä»£ç 

```python
from qdrant_client import QdrantClient
from qdrant_client.http import models

def create_knowledge_units_collection(client: QdrantClient):
    """åˆ›å»ºçŸ¥è¯†å•å…ƒé›†åˆ"""
    
    collection_config = models.CreateCollection(
        collection_name="knowledge_units",
        vectors_config=models.VectorParams(
            size=1536,
            distance=models.Distance.COSINE,
            on_disk_payload=True
        ),
        optimizers_config=models.OptimizersConfig(
            default_segment_number=2,
            max_search_threads=4,
            indexing_threshold=20000
        ),
        hnsw_config=models.HnswConfig(
            m=16,
            ef_construct=100,
            full_scan_threshold=10000
        ),
        wal_config=models.WalConfig(
            wal_capacity_mb=32,
            wal_segments_ahead=4
        ),
        payload_schema={
            "metadata.product_line.soc_type": models.KeywordIndex(),
            "metadata.product_line.firmware_stack": models.KeywordIndex(),
            "metadata.product_line.chipset": models.KeywordIndex(),
            "metadata.product_line.platform": models.KeywordIndex(),
            "metadata.test_context.test_environment": models.KeywordIndex(),
            "metadata.execution_result.status": models.KeywordIndex(),
            "metadata.tags": models.KeywordIndex(),
            "metadata.priority": models.KeywordIndex(),
            "metadata.execution_result.execution_time": models.DatetimeIndex(),
            "metadata.confidence_score": models.FloatIndex()
        }
    )
    
    client.create_collection(collection_config)
    
# ä½¿ç”¨ç¤ºä¾‹
# client = QdrantClient(host="localhost", port=6333)
# create_knowledge_units_collection(client)
```

---

## 5. PostgreSQL å…³ç³»è¡¨ç»“æ„

### 5.1 å»ºè¡¨SQL

```sql
-- çŸ¥è¯†å•å…ƒä¸»è¡¨
CREATE TABLE knowledge_units (
    id VARCHAR(20) PRIMARY KEY,  -- ku_YYYYMMDD_NNN
    title VARCHAR(200) NOT NULL,
    summary TEXT,
    description TEXT NOT NULL,
    change_type VARCHAR(50),
    files_modified TEXT[],
    lines_added INTEGER DEFAULT 0,
    lines_removed INTEGER DEFAULT 0,
    content_vector VECTOR(1536),  -- ä½¿ç”¨ pgvector æ‰©å±•
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    version VARCHAR(10) DEFAULT '1.0',
    source VARCHAR(50) DEFAULT 'automated_extraction',
    author VARCHAR(100),
    confidence_score DECIMAL(3,2),
    metadata JSONB,
    CONSTRAINT ku_id_format CHECK (id ~ '^ku_[0-9]{8}_[0-9]{3}$')
);

-- äº§å“çº¿ä¿¡æ¯è¡¨
CREATE TABLE product_lines (
    id SERIAL PRIMARY KEY,
    soc_type VARCHAR(100) NOT NULL,
    firmware_stack VARCHAR(100) NOT NULL,
    chipset VARCHAR(100),
    platform VARCHAR(50) NOT NULL,
    manufacturer VARCHAR(100),
    launch_year INTEGER,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    UNIQUE(soc_type, firmware_stack, platform)
);

-- ä»£ç ç‰‡æ®µè¡¨
CREATE TABLE code_snippets (
    id SERIAL PRIMARY KEY,
    knowledge_unit_id VARCHAR(20) REFERENCES knowledge_units(id) ON DELETE CASCADE,
    file_path TEXT NOT NULL,
    function_name VARCHAR(200),
    language VARCHAR(50) NOT NULL,
    content TEXT NOT NULL,
    line_start INTEGER,
    line_end INTEGER,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- æµ‹è¯•æ‰§è¡Œè®°å½•è¡¨
CREATE TABLE test_executions (
    id VARCHAR(20) PRIMARY KEY,  -- te_YYYYMMDD_NNN
    knowledge_unit_id VARCHAR(20) REFERENCES knowledge_units(id),
    test_environment VARCHAR(100) NOT NULL,
    test_board VARCHAR(100),
    test_duration VARCHAR(50),
    pass_criteria TEXT,
    execution_status VARCHAR(20) NOT NULL,
    execution_time TIMESTAMP WITH TIME ZONE,
    iterations_count INTEGER DEFAULT 1,
    success_rate DECIMAL(5,4),
    test_results JSONB,
    artifacts_path TEXT[],
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    CONSTRAINT te_id_format CHECK (id ~ '^te_[0-9]{8}_[0-9]{3}$'),
    CONSTRAINT valid_status CHECK (execution_status IN ('success', 'failure', 'timeout', 'error'))
);

-- è¿­ä»£è®°å½•è¡¨
CREATE TABLE iteration_records (
    id SERIAL PRIMARY KEY,
    knowledge_unit_id VARCHAR(20) REFERENCES knowledge_units(id) ON DELETE CASCADE,
    iteration_number INTEGER NOT NULL,
    modification_summary TEXT,
    test_result_status VARCHAR(20),
    analysis_conclusion TEXT,
    decision_made VARCHAR(50),
    execution_duration INTERVAL,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- æ ‡ç­¾è¡¨
CREATE TABLE tags (
    id SERIAL PRIMARY KEY,
    name VARCHAR(100) UNIQUE NOT NULL,
    category VARCHAR(50),  -- domain, component, priority, etc.
    description TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- çŸ¥è¯†å•å…ƒæ ‡ç­¾å…³è”è¡¨
CREATE TABLE knowledge_unit_tags (
    knowledge_unit_id VARCHAR(20) REFERENCES knowledge_units(id) ON DELETE CASCADE,
    tag_id INTEGER REFERENCES tags(id) ON DELETE CASCADE,
    PRIMARY KEY (knowledge_unit_id, tag_id)
);

-- çŸ¥è¯†å•å…ƒå…³è”è¡¨
CREATE TABLE knowledge_unit_relations (
    id SERIAL PRIMARY KEY,
    source_unit_id VARCHAR(20) REFERENCES knowledge_units(id) ON DELETE CASCADE,
    target_unit_id VARCHAR(20) REFERENCES knowledge_units(id) ON DELETE CASCADE,
    relation_type VARCHAR(50) NOT NULL,  -- similar, derived,æ¥è§£å†³, related
    similarity_score DECIMAL(5,4),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    CONSTRAINT valid_relation_type CHECK (relation_type IN ('similar', 'derived', 'solves', 'related', 'depends_on'))
);

-- å¤–éƒ¨é—®é¢˜è·Ÿè¸ªè¡¨
CREATE TABLE external_issues (
    id VARCHAR(20) PRIMARY KEY,  -- issue_YYYYMMDD_NNN
    knowledge_unit_id VARCHAR(20) REFERENCES knowledge_units(id),
    issue_type VARCHAR(50),  -- bug, feature, enhancement
    priority VARCHAR(20),
    status VARCHAR(20),
    redmine_id INTEGER,  -- Redmineé›†æˆID
    jira_key VARCHAR(50),  # Jiraé›†æˆkey
    title VARCHAR(200),
    description TEXT,
    reported_at TIMESTAMP WITH TIME ZONE,
    resolved_at TIMESTAMP WITH TIME ZONE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    CONSTRAINT issue_id_format CHECK (id ~ '^issue_[0-9]{8}_[0-9]{3}$')
);
```

### 5.2 ç´¢å¼•åˆ›å»º

```sql
-- å‘é‡æœç´¢ç´¢å¼•
CREATE INDEX ON knowledge_units USING ivfflat (content_vector vector_cosine_ops)
    WITH (lists = 100);

-- äº§å“çº¿ç»„åˆç´¢å¼•
CREATE INDEX idx_ku_product_line ON knowledge_units 
    USING GIN ((metadata->'product_line'));

-- æµ‹è¯•çŠ¶æ€ç´¢å¼•
CREATE INDEX idx_test_executions_status ON test_executions (execution_status);
CREATE INDEX idx_test_executions_time ON test_executions (execution_time);

-- æ ‡ç­¾ç´¢å¼•
CREATE INDEX idx_ku_tags ON knowledge_unit_tags (tag_id);
CREATE INDEX idx_tags_category ON tags (category);

-- æ—¶é—´èŒƒå›´ç´¢å¼•
CREATE INDEX idx_ku_created_time ON knowledge_units (created_at);
CREATE INDEX idx_relations_created ON knowledge_unit_relations (created_at);

-- å…¨æ–‡æœç´¢ç´¢å¼•
CREATE INDEX idx_ku_search ON knowledge_units 
    USING GIN (to_tsvector('english', title || ' ' || description));

-- JSONBå­—æ®µç´¢å¼•
CREATE INDEX idx_ku_metadata ON knowledge_units USING GIN (metadata);
CREATE INDEX idx_test_results ON test_executions USING GIN (test_results);
```

### 5.3 è§†å›¾åˆ›å»º

```sql
-- çŸ¥è¯†å•å…ƒè¯¦ç»†è§†å›¾
CREATE VIEW knowledge_units_detailed AS
SELECT 
    ku.id,
    ku.title,
    ku.summary,
    ku.description,
    ku.change_type,
    ku.files_modified,
    ku.lines_added,
    ku.lines_removed,
    ku.author,
    ku.confidence_score,
    ku.metadata,
    ku.created_at,
    pl.soc_type,
    pl.firmware_stack,
    pl.chipset,
    pl.platform,
    ku.execution_status,
    ku.success_rate,
    array_agg(DISTINCT t.name) as tags
FROM knowledge_units ku
LEFT JOIN product_lines pl ON pl.id = ku.product_line_id
LEFT JOIN knowledge_unit_tags kut ON kut.knowledge_unit_id = ku.id
LEFT JOIN tags t ON t.id = kut.tag_id
GROUP BY ku.id, pl.soc_type, pl.firmware_stack, pl.chipset, pl.platform;

-- æµ‹è¯•æ‰§è¡Œç»Ÿè®¡è§†å›¾
CREATE VIEW test_execution_stats AS
SELECT 
    DATE_TRUNC('day', execution_time) as date,
    test_environment,
    execution_status,
    COUNT(*) as count,
    AVG(iterations_count) as avg_iterations,
    AVG(success_rate) as avg_success_rate
FROM test_executions
WHERE execution_time >= CURRENT_DATE - INTERVAL '30 days'
GROUP BY date, test_environment, execution_status
ORDER BY date DESC, test_environment;

-- æ ‡ç­¾ä½¿ç”¨ç»Ÿè®¡è§†å›¾
CREATE VIEW tag_usage_stats AS
SELECT 
    t.name,
    t.category,
    COUNT(kut.knowledge_unit_id) as usage_count,
    AVG(ku.confidence_score) as avg_confidence
FROM tags t
JOIN knowledge_unit_tags kut ON kut.tag_id = t.id
JOIN knowledge_units ku ON ku.id = kut.knowledge_unit_id
GROUP BY t.id, t.name, t.category
ORDER BY usage_count DESC;
```

---

## 6. æŸ¥è¯¢ç¤ºä¾‹

### 6.1 Qdrant å‘é‡æ£€ç´¢æŸ¥è¯¢

```python
import numpy as np
from qdrant_client import QdrantClient
from qdrant_client.http import models

def semantic_search_knowledge_units(client, query_text, limit=10):
    """åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦çš„çŸ¥è¯†åº“æ£€ç´¢"""
    
    # ç”ŸæˆæŸ¥è¯¢å‘é‡ï¼ˆå®é™…ä½¿ç”¨æ—¶æ¥å…¥embeddingæ¨¡å‹ï¼‰
    query_vector = generate_embedding(query_text)  # [0.1, -0.2, ...]
    
    search_result = client.search(
        collection_name="knowledge_units",
        query_vector=query_vector,
        query_filter=models.Filter(
            must=[
                models.FieldCondition(
                    key="metadata.product_line.soc_type",
                    match=models.MatchValue(value="Tiger_Lake")
                ),
                models.FieldCondition(
                    key="metadata.execution_result.status",
                    match=models.MatchValue(value="success")
                )
            ]
        ),
        limit=limit,
        with_payload=True,
        with_vectors=False
    )
    
    return search_result

def advanced_hybrid_search(client, query_text, filters, limit=20):
    """æ··åˆæ£€ç´¢ï¼šå‘é‡ + å…ƒæ•°æ®è¿‡æ»¤"""
    
    query_vector = generate_embedding(query_text)
    
    # æ„å»ºå¤æ‚çš„è¿‡æ»¤æ¡ä»¶
    must_conditions = []
    
    if filters.get('soc_types'):
        must_conditions.append(
            models.FieldCondition(
                key="metadata.product_line.soc_type",
                match=models.MatchAny(any=filters['soc_types'])
            )
        )
    
    if filters.get('firmware_stacks'):
        must_conditions.append(
            models.FieldCondition(
                key="metadata.product_line.firmware_stack",
                match=models.MatchAny(any=filters['firmware_stacks'])
            )
        )
    
    if filters.get('min_confidence'):
        must_conditions.append(
            models.RangeCondition(
                key="metadata.confidence_score",
                gte=filters['min_confidence']
            )
        )
    
    # æ·»åŠ æ—¥æœŸèŒƒå›´è¿‡æ»¤
    if filters.get('date_from'):
        must_conditions.append(
            models.FieldCondition(
                key="metadata.execution_result.execution_time",
                match=models.MatchDatetime(gt=filters['date_from'])
            )
        )
    
    search_result = client.search(
        collection_name="knowledge_units",
        query_vector=query_vector,
        query_filter=models.Filter(must=must_conditions) if must_conditions else None,
        limit=limit,
        with_payload=True,
        with_vectors=False,
        score_threshold=0.7  # æœ€ä½ç›¸ä¼¼åº¦é˜ˆå€¼
    )
    
    return search_result

# ä½¿ç”¨ç¤ºä¾‹
client = QdrantClient(host="localhost", port=6333)

# 1. åŸºç¡€è¯­ä¹‰æœç´¢
results = semantic_search_knowledge_units(
    client, 
    "PCIe device enumeration timing issues",
    limit=5
)

# 2. é«˜çº§æ··åˆæœç´¢
advanced_results = advanced_hybrid_search(
    client,
    "UEFI initialization problems",
    {
        'soc_types': ['Tiger_Lake', 'Alder_Lake'],
        'firmware_stacks': ['UEFI_2.8', 'UEFI_2.9'],
        'min_confidence': 0.8,
        'date_from': '2024-01-01T00:00:00Z'
    },
    limit=10
)
```

### 6.2 PostgreSQL å¤æ‚æŸ¥è¯¢ç¤ºä¾‹

```sql
-- 1. æŸ¥æ‰¾ç‰¹å®šäº§å“çº¿çš„é«˜ç½®ä¿¡åº¦è§£å†³æ–¹æ¡ˆ
SELECT 
    ku.id,
    ku.title,
    ku.summary,
    ku.confidence_score,
    ku.created_at,
    pl.soc_type,
    pl.firmware_stack,
    array_agg(DISTINCT t.name) as relevant_tags
FROM knowledge_units ku
JOIN product_lines pl ON (
    ku.metadata->'product_line'->>'soc_type' = pl.soc_type 
    AND ku.metadata->'product_line'->>'firmware_stack' = pl.firmware_stack
)
LEFT JOIN knowledge_unit_tags kut ON kut.knowledge_unit_id = ku.id
LEFT JOIN tags t ON t.id = kut.tag_id
WHERE pl.soc_type = 'Tiger_Lake'
  AND pl.firmware_stack = 'UEFI_2.8'
  AND ku.confidence_score >= 0.85
  AND ku.metadata->'execution_result'->>'status' = 'success'
GROUP BY ku.id, ku.title, ku.summary, ku.confidence_score, ku.created_at, pl.soc_type, pl.firmware_stack
ORDER BY ku.confidence_score DESC, ku.created_at DESC
LIMIT 10;

-- 2. åˆ†ææµ‹è¯•æ‰§è¡Œè¶‹åŠ¿
WITH test_trends AS (
    SELECT 
        DATE_TRUNC('week', te.execution_time) as week,
        te.test_environment,
        COUNT(*) as total_executions,
        COUNT(*) FILTER (WHERE te.execution_status = 'success') as successful_executions,
        ROUND(
            COUNT(*) FILTER (WHERE te.execution_status = 'success') * 100.0 / COUNT(*), 2
        ) as success_rate_percent
    FROM test_executions te
    WHERE te.execution_time >= CURRENT_DATE - INTERVAL '12 weeks'
      AND te.knowledge_unit_id IS NOT NULL
    GROUP BY week, te.test_environment
)
SELECT 
    week,
    test_environment,
    total_executions,
    successful_executions,
    success_rate_percent,
    LAG(success_rate_percent) OVER (PARTITION BY test_environment ORDER BY week) as prev_week_success_rate,
    success_rate_percent - LAG(success_rate_percent) OVER (PARTITION BY test_environment ORDER BY week) as success_rate_change
FROM test_trends
ORDER BY week DESC, test_environment;

-- 3. æŸ¥æ‰¾ç›¸å…³çŸ¥è¯†å•å…ƒ
WITH similar_units AS (
    SELECT DISTINCT
        ku1.id as source_unit,
        ku2.id as target_unit,
        ku2.title,
        ku2.summary,
        ku2.confidence_score,
        -- åŸºäºæ ‡ç­¾é‡å è®¡ç®—ç›¸ä¼¼åº¦
        (
            SELECT COUNT(*) * 1.0 / 
                   GREATEST(
                       (SELECT COUNT(*) FROM knowledge_unit_tags WHERE knowledge_unit_id = ku1.id),
                       (SELECT COUNT(*) FROM knowledge_unit_tags WHERE knowledge_unit_id = ku2.id)
                   )
            FROM knowledge_unit_tags kut1
            JOIN knowledge_unit_tags kut2 ON kut1.tag_id = kut2.tag_id
            WHERE kut1.knowledge_unit_id = ku1.id 
              AND kut2.knowledge_unit_id = ku2.id
        ) as tag_similarity
    FROM knowledge_units ku1
    JOIN knowledge_unit_tags kut1 ON kut1.knowledge_unit_id = ku1.id
    JOIN knowledge_unit_tags kut2 ON kut2.tag_id = kut1.tag_id
    JOIN knowledge_units ku2 ON ku2.id = kut2.knowledge_unit_id
    WHERE ku1.id = 'ku_20241227_001'
      AND ku2.id != 'ku_20241227_001'
      AND ku2.metadata->'execution_result'->>'status' = 'success'
)
SELECT 
    target_unit,
    title,
    summary,
    confidence_score,
    tag_similarity,
    ROUND(tag_similarity * 100, 2) as similarity_percent
FROM similar_units
WHERE tag_similarity >= 0.3
ORDER BY tag_similarity DESC, confidence_score DESC
LIMIT 5;

-- 4. äº§å“çº¿é—®é¢˜åˆ†å¸ƒåˆ†æ
SELECT 
    pl.soc_type,
    pl.firmware_stack,
    COUNT(ku.id) as total_issues,
    COUNT(ku.id) FILTER (WHERE ku.metadata->'execution_result'->>'status' = 'success') as resolved_issues,
    COUNT(ku.id) FILTER (WHERE ku.metadata->'execution_result'->>'status' = 'failure') as unresolved_issues,
    ROUND(
        COUNT(ku.id) FILTER (WHERE ku.metadata->'execution_result'->>'status' = 'success') * 100.0 / 
        NULLIF(COUNT(ku.id), 0), 2
    ) as resolution_rate_percent,
    AVG(ku.confidence_score) as avg_confidence
FROM product_lines pl
LEFT JOIN knowledge_units ku ON (
    ku.metadata->'product_line'->>'soc_type' = pl.soc_type 
    AND ku.metadata->'product_line'->>'firmware_stack' = pl.firmware_stack
)
GROUP BY pl.id, pl.soc_type, pl.firmware_stack
HAVING COUNT(ku.id) > 0
ORDER BY resolution_rate_percent DESC, total_issues DESC;

-- 5. ä»£ç ä¿®æ”¹æ¨¡å¼åˆ†æ
SELECT 
    change_type,
    COUNT(*) as change_count,
    AVG(lines_added) as avg_lines_added,
    AVG(lines_removed) as avg_lines_removed,
    AVG(confidence_score) as avg_confidence,
    array_agg(DISTINCT soc_type) as affected_soc_types
FROM (
    SELECT 
        ku.change_type,
        ku.lines_added,
        ku.lines_removed,
        ku.confidence_score,
        ku.metadata->'product_line'->>'soc_type' as soc_type
    FROM knowledge_units ku
    WHERE ku.change_type IS NOT NULL
      AND ku.created_at >= CURRENT_DATE - INTERVAL '3 months'
) subquery
GROUP BY change_type
ORDER BY change_count DESC;

-- 6. æœ€è¿‘æ´»è·ƒçš„çŸ¥è¯†å•å…ƒ
SELECT 
    ku.id,
    ku.title,
    ku.summary,
    ku.confidence_score,
    ku.created_at,
    ku.updated_at,
    ku.metadata->'product_line'->>'soc_type' as soc_type,
    ku.metadata->'execution_result'->>'status' as status,
    -- æ£€æŸ¥æ˜¯å¦æœ‰æœ€è¿‘çš„å…³è”æµ‹è¯•
    CASE 
        WHEN EXISTS (
            SELECT 1 FROM test_executions te 
            WHERE te.knowledge_unit_id = ku.id 
            AND te.created_at >= CURRENT_DATE - INTERVAL '7 days'
        ) THEN 'recently_tested'
        ELSE 'no_recent_test'
    END as test_status
FROM knowledge_units ku
WHERE ku.created_at >= CURRENT_DATE - INTERVAL '30 days'
  AND ku.metadata->'execution_result'->>'status' = 'success'
ORDER BY ku.updated_at DESC, ku.confidence_score DESC
LIMIT 15;
```

### 6.3 ç»„åˆæŸ¥è¯¢ï¼šå‘é‡æ£€ç´¢ + å…³ç³»æ•°æ®

```python
import psycopg2
from qdrant_client import QdrantClient

def comprehensive_search(query_text, soc_type=None, min_confidence=0.7, limit=10):
    """ç»¼åˆæ£€ç´¢ï¼šå…ˆå‘é‡æœç´¢ï¼Œå†å…³ç³»æ•°æ®è¿‡æ»¤å’Œè¡¥å……"""
    
    qdrant_client = QdrantClient(host="localhost", port=6333)
    
    # æ­¥éª¤1ï¼šå‘é‡æ£€ç´¢å€™é€‰ç»“æœ
    vector_results = qdrant_client.search(
        collection_name="knowledge_units",
        query_vector=generate_embedding(query_text),
        limit=limit * 2,  # è·å–æ›´å¤šå€™é€‰ç»“æœè¿›è¡Œåç»­è¿‡æ»¤
        with_payload=True
    )
    
    if not vector_results:
        return []
    
    # æ­¥éª¤2ï¼šæå–å€™é€‰IDs
    candidate_ids = [r.id for r in vector_results]
    
    # æ­¥éª¤3ï¼šPostgreSQLå…³ç³»æ•°æ®è¡¥å……
    conn = psycopg2.connect(
        host="localhost",
        database="knowledge_db", 
        user="kb_user",
        password="kb_password"
    )
    
    with conn.cursor() as cur:
        # è·å–å®Œæ•´ä¿¡æ¯
        query = """
        SELECT 
            ku.id,
            ku.title,
            ku.summary,
            ku.description,
            ku.confidence_score,
            ku.metadata,
            ku.created_at,
            pl.soc_type,
            pl.firmware_stack,
            ku.metadata->'execution_result'->>'status' as status,
            array_agg(DISTINCT t.name) as tags
        FROM knowledge_units ku
        LEFT JOIN product_lines pl ON (
            ku.metadata->'product_line'->>'soc_type' = pl.soc_type 
            AND ku.metadata->'product_line'->>'firmware_stack' = pl.firmware_stack
        )
        LEFT JOIN knowledge_unit_tags kut ON kut.knowledge_unit_id = ku.id
        LEFT JOIN tags t ON t.id = kut.tag_id
        WHERE ku.id = ANY(%s)
        """
        
        params = [candidate_ids]
        
        if soc_type:
            query += " AND pl.soc_type = %s"
            params.append(soc_type)
        
        if min_confidence:
            query += " AND ku.confidence_score >= %s"
            params.append(min_confidence)
        
        query += """
        GROUP BY ku.id, ku.title, ku.summary, ku.description, 
                 ku.confidence_score, ku.metadata, ku.created_at, 
                 pl.soc_type, pl.firmware_stack
        ORDER BY ku.confidence_score DESC, ku.created_at DESC
        LIMIT %s
        """
        params.append(limit)
        
        cur.execute(query, params)
        results = cur.fetchall()
    
    conn.close()
    
    # æ­¥éª¤4ï¼šç»“åˆå‘é‡ç›¸ä¼¼åº¦å’Œå…³ç³»æ•°æ®ç½®ä¿¡åº¦è¿›è¡Œæœ€ç»ˆæ’åº
    enhanced_results = []
    for result in results:
        ku_id = result[0]
        
        # æ‰¾åˆ°å¯¹åº”çš„å‘é‡æ£€ç´¢ç»“æœ
        vector_result = next((r for r in vector_results if r.id == ku_id), None)
        if vector_result:
            enhanced_results.append({
                'id': ku_id,
                'title': result[1],
                'summary': result[2],
                'description': result[3],
                'confidence_score': result[4],
                'metadata': result[5],
                'created_at': result[6],
                'soc_type': result[7],
                'firmware_stack': result[8],
                'status': result[9],
                'tags': result[10] or [],
                'vector_similarity': vector_result.score,  # å‘é‡ç›¸ä¼¼åº¦
                'combined_score': (result[4] + vector_result.score) / 2  # ç»„åˆå¾—åˆ†
            })
    
    # æŒ‰ç»„åˆå¾—åˆ†é‡æ–°æ’åº
    enhanced_results.sort(key=lambda x: x['combined_score'], reverse=True)
    
    return enhanced_results[:limit]

# ä½¿ç”¨ç¤ºä¾‹
results = comprehensive_search(
    query_text="PCIe initialization and device enumeration",
    soc_type="Tiger_Lake", 
    min_confidence=0.8,
    limit=5
)

for result in results:
    print(f"ID: {result['id']}")
    print(f"Title: {result['title']}")
    print(f"Confidence: {result['confidence_score']:.2f}")
    print(f"Vector Similarity: {result['vector_similarity']:.3f}")
    print(f"Combined Score: {result['combined_score']:.3f}")
    print(f"Tags: {', '.join(result['tags'])}")
    print("-" * 50)
```

---

## 7. æ•°æ®è¿ç§»å’Œç»´æŠ¤

### 7.1 æ•°æ®å¯¼å…¥è„šæœ¬

```python
import json
import uuid
from datetime import datetime
from qdrant_client import QdrantClient
import psycopg2

def import_knowledge_units_from_json(file_path):
    """ä»JSONæ–‡ä»¶æ‰¹é‡å¯¼å…¥çŸ¥è¯†å•å…ƒ"""
    
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    qdrant_client = QdrantClient(host="localhost", port=6333)
    
    # PostgreSQLè¿æ¥
    pg_conn = psycopg2.connect(
        host="localhost",
        database="knowledge_db",
        user="kb_user", 
        password="kb_password"
    )
    
    try:
        for item in data:
            ku_id = item['id']
            
            # 1. æ’å…¥PostgreSQL
            insert_postgresql_knowledge_unit(pg_conn, item)
            
            # 2. æ’å…¥Qdrant
            insert_qdrant_knowledge_unit(qdrant_client, item)
            
            print(f"âœ… å¯¼å…¥çŸ¥è¯†å•å…ƒ: {ku_id}")
            
        pg_conn.commit()
        print(f"ğŸ‰ æˆåŠŸå¯¼å…¥ {len(data)} ä¸ªçŸ¥è¯†å•å…ƒ")
        
    except Exception as e:
        pg_conn.rollback()
        print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
        raise
    finally:
        pg_conn.close()

def insert_postgresql_knowledge_unit(conn, ku_data):
    """æ’å…¥å•ä¸ªçŸ¥è¯†å•å…ƒåˆ°PostgreSQL"""
    with conn.cursor() as cur:
        # æ’å…¥knowledge_unitsè¡¨
        cur.execute("""
            INSERT INTO knowledge_units (
                id, title, summary, description, change_type, files_modified,
                lines_added, lines_removed, author, confidence_score,
                metadata, created_at, updated_at
            ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
            ON CONFLICT (id) DO UPDATE SET
                title = EXCLUDED.title,
                summary = EXCLUDED.summary, 
                description = EXCLUDED.description,
                metadata = EXCLUDED.metadata,
                updated_at = EXCLUDED.updated_at
        """, (
            ku_data['id'],
            ku_data['content']['title'],
            ku_data['content']['summary'],
            ku_data['content']['description'],
            ku_data['content']['modification_details']['change_type'],
            ku_data['content']['modification_details']['files_modified'],
            ku_data['content']['modification_details']['lines_added'],
            ku_data['content']['modification_details']['lines_removed'],
            ku_data['metadata']['author'],
            ku_data['metadata']['confidence_score'],
            json.dumps(ku_data['metadata']),
            ku_data['audit']['created_at'],
            ku_data['audit']['updated_at']
        ))
        
        # æ’å…¥ä»£ç ç‰‡æ®µ
        for snippet in ku_data['content']['code_snippets']:
            cur.execute("""
                INSERT INTO code_snippets (
                    knowledge_unit_id, file_path, function_name, language, content
                ) VALUES (%s, %s, %s, %s, %s)
                ON CONFLICT DO NOTHING
            """, (
                ku_data['id'],
                snippet['file_path'],
                snippet.get('function'),
                snippet['language'],
                snippet['content']
            ))

def insert_qdrant_knowledge_unit(client, ku_data):
    """æ’å…¥å•ä¸ªçŸ¥è¯†å•å…ƒåˆ°Qdrant"""
    
    client.upsert(
        collection_name="knowledge_units",
        points=[models.PointStruct(
            id=ku_data['id'],
            vector=ku_data['vector_embedding']['vector'],
            payload={
                'metadata': ku_data['metadata'],
                'title': ku_data['content']['title'],
                'summary': ku_data['content']['summary']
            }
        )]
    )

# æ‰§è¡Œå¯¼å…¥
# import_knowledge_units_from_json('/data/knowledge_units_export.json')
```

### 7.2 æ•°æ®ç»´æŠ¤è„šæœ¬

```python
def cleanup_orphaned_data():
    """æ¸…ç†å­¤ç«‹çš„æµ‹è¯•æ‰§è¡Œè®°å½•å’Œå…³è”æ•°æ®"""
    
    conn = psycopg2.connect(
        host="localhost",
        database="knowledge_db",
        user="kb_user",
        password="kb_password"
    )
    
    try:
        with conn.cursor() as cur:
            # åˆ é™¤æ²¡æœ‰çŸ¥è¯†å•å…ƒå…³è”çš„æµ‹è¯•æ‰§è¡Œè®°å½•
            cur.execute("""
                DELETE FROM test_executions 
                WHERE knowledge_unit_id IS NOT NULL 
                AND knowledge_unit_id NOT IN (SELECT id FROM knowledge_units)
            """)
            
            # åˆ é™¤æ²¡æœ‰çŸ¥è¯†å•å…ƒå…³è”çš„æ ‡ç­¾å…³è”
            cur.execute("""
                DELETE FROM knowledge_unit_tags 
                WHERE knowledge_unit_id NOT IN (SELECT id FROM knowledge_units)
            """)
            
            # åˆ é™¤æ²¡æœ‰çŸ¥è¯†å•å…ƒå…³è”çš„è¿­ä»£è®°å½•
            cur.execute("""
                DELETE FROM iteration_records 
                WHERE knowledge_unit_id NOT IN (SELECT id FROM knowledge_units)
            """)
            
            deleted_count = cur.rowcount
            print(f"ğŸ§¹ æ¸…ç†äº† {deleted_count} æ¡å­¤ç«‹è®°å½•")
            
        conn.commit()
        
    finally:
        conn.close()

def update_confidence_scores():
    """æ›´æ–°çŸ¥è¯†å•å…ƒçš„ç½®ä¿¡åº¦åˆ†æ•°"""
    
    conn = psycopg2.connect(
        host="localhost", 
        database="knowledge_db",
        user="kb_user",
        password="kb_password"
    )
    
    try:
        with conn.cursor() as cur:
            # åŸºäºæ‰§è¡Œç»“æœæ›´æ–°ç½®ä¿¡åº¦
            cur.execute("""
                UPDATE knowledge_units 
                SET confidence_score = 
                    CASE 
                        WHEN metadata->'execution_result'->>'status' = 'success' THEN 
                            LEAST(0.95, confidence_score + 0.1)
                        WHEN metadata->'execution_result'->>'status' = 'failure' THEN
                            GREATEST(0.1, confidence_score - 0.1)
                        ELSE confidence_score
                    END
                WHERE metadata->'execution_result'->>'status' IS NOT NULL
            """)
            
            updated_count = cur.rowcount
            print(f"ğŸ“Š æ›´æ–°äº† {updated_count} ä¸ªç½®ä¿¡åº¦åˆ†æ•°")
            
        conn.commit()
        
    finally:
        conn.close()

def generate_usage_report():
    """ç”ŸæˆçŸ¥è¯†åº“ä½¿ç”¨æŠ¥å‘Š"""
    
    conn = psycopg2.connect(
        host="localhost",
        database="knowledge_db", 
        user="kb_user",
        password="kb_password"
    )
    
    try:
        with conn.cursor() as cur:
            # æ•´ä½“ç»Ÿè®¡
            cur.execute("SELECT COUNT(*) FROM knowledge_units")
            total_units = cur.fetchone()[0]
            
            cur.execute("SELECT COUNT(*) FROM knowledge_units WHERE confidence_score >= 0.8")
            high_confidence = cur.fetchone()[0]
            
            cur.execute("SELECT COUNT(*) FROM test_executions WHERE execution_status = 'success'")
            successful_tests = cur.fetchone()[0]
            
            print("ğŸ“ˆ çŸ¥è¯†åº“ä½¿ç”¨æŠ¥å‘Š")
            print("=" * 40)
            print(f"æ€»çŸ¥è¯†å•å…ƒæ•°: {total_units}")
            print(f"é«˜ç½®ä¿¡åº¦å•å…ƒ: {high_confidence} ({high_confidence/total_units*100:.1f}%)")
            print(f"æˆåŠŸæµ‹è¯•æ•°: {successful_tests}")
            
            # æŒ‰äº§å“çº¿ç»Ÿè®¡
            cur.execute("""
                SELECT 
                    metadata->'product_line'->>'soc_type' as soc_type,
                    COUNT(*) as count,
                    AVG(confidence_score) as avg_confidence
                FROM knowledge_units
                GROUP BY metadata->'product_line'->>'soc_type'
                ORDER BY count DESC
            """)
            
            print("\nğŸ”§ æŒ‰SoCç±»å‹ç»Ÿè®¡:")
            for row in cur.fetchall():
                print(f"  {row[0]}: {row[1]} å•å…ƒ, å¹³å‡ç½®ä¿¡åº¦: {row[2]:.2f}")
                
    finally:
        conn.close()

# æ‰§è¡Œç»´æŠ¤ä»»åŠ¡
if __name__ == "__main__":
    print("å¼€å§‹æ•°æ®ç»´æŠ¤...")
    cleanup_orphaned_data()
    update_confidence_scores()
    generate_usage_report()
    print("æ•°æ®ç»´æŠ¤å®Œæˆ!")
```

---

## 8. æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 8.1 Qdrant æ€§èƒ½ä¼˜åŒ–

```python
# ä¼˜åŒ–é…ç½®
OPTIMIZATION_CONFIG = {
    # ç´¢å¼•ä¼˜åŒ–
    "indexing_threshold": 20000,  # è‡ªåŠ¨ç´¢å¼•çš„æ¡ç›®é˜ˆå€¼
    "max_optimization_threads": 4,
    
    # å†…å­˜ä¼˜åŒ–  
    "memmap_threshold_kb": 100,  # å†…å­˜æ˜ å°„é˜ˆå€¼
    "max_search_threads": 4,     # å¹¶è¡Œæœç´¢çº¿ç¨‹æ•°
    
    # HNSWå‚æ•°ä¼˜åŒ–
    "hnsw_m": 16,                # è¿æ¥æ•°ï¼Œå½±å“æœç´¢ç²¾åº¦å’Œå†…å­˜
    "hnsw_ef_construct": 100,    # æ„å»ºæ—¶çš„æœç´¢èŒƒå›´
    "hnsw_full_scan_threshold": 10000,  # å…¨è¡¨æ‰«æé˜ˆå€¼
}

# ç›‘æ§æŸ¥è¯¢æ€§èƒ½
def monitor_qdrant_performance(client):
    """ç›‘æ§Qdrantæ€§èƒ½æŒ‡æ ‡"""
    
    collection_info = client.get_collection("knowledge_units")
    
    print("ğŸ“Š Qdrant æ€§èƒ½æŒ‡æ ‡")
    print("=" * 30)
    print(f"å‘é‡æ•°é‡: {collection_info.vectors_count}")
    print(f"ç´¢å¼•çŠ¶æ€: {collection_info.indexed_vectors_count}")
    print(f"æ®µæ•°é‡: {collection_info.segments_count}")
    print(f"çŠ¶æ€: {collection_info.status}")
```

### 8.2 PostgreSQL æ€§èƒ½ä¼˜åŒ–

```sql
-- åˆ†åŒºè¡¨ï¼ˆæŒ‰æœˆåˆ†åŒºï¼‰
CREATE TABLE knowledge_units_partitioned (
    LIKE knowledge_units INCLUDING ALL
) PARTITION BY RANGE (created_at);

-- åˆ›å»ºæœˆåº¦åˆ†åŒº
CREATE TABLE knowledge_units_2024_12 PARTITION OF knowledge_units_partitioned
    FOR VALUES FROM ('2024-12-01') TO ('2025-01-01');

-- æŸ¥è¯¢ä¼˜åŒ–é…ç½®
ALTER SYSTEM SET shared_preload_libraries = 'pg_stat_statements';
ALTER SYSTEM SET max_connections = 200;
ALTER SYSTEM SET shared_buffers = '256MB';
ALTER SYSTEM SET effective_cache_size = '1GB';
ALTER SYSTEM SET work_mem = '4MB';
ALTER SYSTEM SET maintenance_work_mem = '64MB';

-- å¯ç”¨å¹¶è¡ŒæŸ¥è¯¢
ALTER SYSTEM SET max_parallel_workers = 4;
ALTER SYSTEM SET max_parallel_workers_per_gather = 2;
```

---

æœ¬æ–‡æ¡£æä¾›äº†å®Œæ•´çš„çŸ¥è¯†åº“æ•°æ®ç»“æ„è®¾è®¡æ–¹æ¡ˆï¼ŒåŒ…å«300+è¡Œè¯¦ç»†è§„èŒƒï¼Œæ¶µç›–äº†æ•°æ®æ¨¡å‹ã€å­˜å‚¨æ¶æ„ã€æŸ¥è¯¢ç¤ºä¾‹å’Œæ€§èƒ½ä¼˜åŒ–ç­–ç•¥ã€‚è®¾è®¡æ”¯æŒé«˜æ•ˆçš„è¯­ä¹‰æ£€ç´¢ã€ç»“æ„åŒ–æ•°æ®ç®¡ç†å’Œäº§å“çº¿å·®å¼‚åŒ–æŸ¥è¯¢ï¼Œä¸ºAIé©±åŠ¨çš„å›ºä»¶æµ‹è¯•ç³»ç»Ÿæä¾›äº†åšå®çš„æ•°æ®åŸºç¡€ã€‚